# -*- coding: utf-8 -*-
"""Web-2_Analytics_and_Optimization_Project_main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NvllHpS8N1sl8e2HoO5IrQqmvND0eFfY

# A/B TESTING FRAMEWORK
## Advanced statistical methods for A/B testing

### Author: Peter Chika ozo-ogueji
"""

# Web Analytics & Optimization Project - Dataset Downloader
# For Google Colab Environment - REAL DATASETS ONLY

import os
import json
import pandas as pd
import zipfile
import requests
import numpy as np
from pathlib import Path

# =============================================================================
# STEP 1: SETUP KAGGLE API IN GOOGLE COLAB
# =============================================================================

def setup_kaggle_api():
    """Setup Kaggle API credentials in Google Colab"""

    # Your Kaggle credentials
    kaggle_credentials = {
        "username": "peterchikaozoogueji",
        "key": "f2b5ae97165cf3eef611db7624db7192"
    }

    # Create .kaggle directory
    os.makedirs('/root/.kaggle', exist_ok=True)

    # Write credentials to kaggle.json
    with open('/root/.kaggle/kaggle.json', 'w') as f:
        json.dump(kaggle_credentials, f)

    # Set proper permissions
    os.chmod('/root/.kaggle/kaggle.json', 0o600)

    print("âœ… Kaggle API credentials setup complete!")

# =============================================================================
# STEP 2: INSTALL REQUIRED PACKAGES
# =============================================================================

def install_packages():
    """Install required packages for the project"""
    packages = [
        'kaggle',
        'plotly',
        'streamlit',
        'scipy',
        'scikit-learn',
        'seaborn',
        'openpyxl',  # For Excel file support
        'datasets',  # For Hugging Face datasets
        'timeout-decorator'  # For handling timeouts
    ]

    for package in packages:
        os.system(f'pip install {package}')

    print("âœ… All packages installed!")

# =============================================================================
# STEP 3: DEFINE WORKING DATASETS (VERIFIED REAL DATASETS)
# =============================================================================

# Primary datasets that work
WORKING_DATASETS = {
    # E-commerce Transaction Data (VERIFIED WORKING)
    'olist_ecommerce': 'olistbr/brazilian-ecommerce',
    'online_retail': 'lakshmi25npathi/online-retail-dataset',

    # A/B Testing Data (VERIFIED WORKING)
    'ab_testing': 'amirmotefaker/ab-testing-dataset',
    'cookie_cats': 'yufengsui/mobile-games-ab-testing',

    # Marketing & Campaign Data (VERIFIED WORKING)
    'marketing_campaign': 'rodsaldanha/arketing-campaign',
    'customer_personality': 'imakash3011/customer-personality-analysis',

    # Product Data (VERIFIED WORKING)
    'amazon_products': 'asaniczka/amazon-products-dataset-2023-1-4m-products',
    'ecommerce_behavior': 'mkechinov/ecommerce-behavior-data-from-multi-category-store',

    # Retail Analytics (VERIFIED WORKING)
    'retail_analytics': 'manjeetsingh/retaildataset'
}

# High-quality replacement datasets for problematic ones (VERIFIED WORKING)
REPLACEMENT_DATASETS = {
    # Web Analytics & Session Data Replacements (WORKING)
    'digital_ads_real': 'loveall/clicks-conversion-tracking',
    'ecommerce_events': 'mkechinov/ecommerce-events-history-in-cosmetics-shop',

    # Additional Working Datasets
    'ecommerce_sales': 'carrie1/ecommerce-data',
    'online_sales': 'mohammadtalib786/retail-sales-dataset',
    'customer_shopping': 'mehmettahiraslan/customer-shopping-dataset',
    'web_classification': 'ruchi798/website-classification',
    'facebook_ads': 'madislemsalu/facebook-ad-campaign-dataset',
    'advertising_data': 'sazid28/advertising-dataset',
    'retail_sales': 'ankitbansal06/retail-orders',
    'user_behavior': 'henrysue/e-commerce-user-behavior-dataset',
    'shopping_trends': 'iamsouravbanerjee/customer-shopping-trends-dataset'
}

# GitHub datasets (direct download) - WORKING URLS
GITHUB_DATASETS = {
    'ecommerce_github': 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/Ecdat/Clothing.csv',
    'retail_transactions': 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/Ecdat/Fair.csv',
    'consumer_data': 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/Ecdat/BudgetFood.csv'
}

# Hugging Face datasets (using datasets library)
HUGGINGFACE_DATASETS = {
    'ecommerce_hf': 'ecommerce_behavior',
    'retail_hf': 'retail_rocket',
    'web_analytics_hf': 'web_clicks'
}

# =============================================================================
# STEP 4: DOWNLOAD FUNCTIONS
# =============================================================================

def download_kaggle_dataset(dataset_name, dataset_path, folder_name):
    """Download a specific Kaggle dataset"""
    try:
        print(f"ğŸ“¥ Downloading {dataset_name}...")

        # Create directory for dataset
        os.makedirs(f'/content/datasets/{folder_name}', exist_ok=True)

        # Download dataset
        os.system(f'kaggle datasets download -d {dataset_path} -p /content/datasets/{folder_name}')

        # Extract if zip file exists
        zip_files = list(Path(f'/content/datasets/{folder_name}').glob('*.zip'))
        for zip_file in zip_files:
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                zip_ref.extractall(f'/content/datasets/{folder_name}')
            # Remove zip file after extraction
            os.remove(zip_file)

        print(f"âœ… {dataset_name} downloaded successfully!")
        return True

    except Exception as e:
        print(f"âŒ Failed to download {dataset_name}: {str(e)}")
        return False

def download_github_dataset(dataset_name, url, filename):
    """Download dataset directly from GitHub"""
    try:
        print(f"ğŸ“¥ Downloading {dataset_name} from GitHub...")

        os.makedirs(f'/content/datasets/{dataset_name}', exist_ok=True)

        response = requests.get(url)
        response.raise_for_status()

        file_path = f'/content/datasets/{dataset_name}/{filename}'
        with open(file_path, 'wb') as f:
            f.write(response.content)

        print(f"âœ… {dataset_name} downloaded from GitHub!")
        return True

    except Exception as e:
        print(f"âŒ Failed to download {dataset_name} from GitHub: {str(e)}")
        return False

def download_all_working_datasets():
    """Download all verified working datasets"""
    print("ğŸš€ Starting download of WORKING datasets...\n")

    success_count = 0
    total_datasets = len(WORKING_DATASETS)

    for dataset_name, dataset_path in WORKING_DATASETS.items():
        success = download_kaggle_dataset(dataset_name, dataset_path, dataset_name)
        if success:
            success_count += 1
        print("-" * 50)

    print(f"\nğŸ“Š Working Datasets Summary: {success_count}/{total_datasets} downloaded successfully")
    return success_count

def download_replacement_datasets():
    """Download high-quality replacement datasets"""
    print("\nğŸ”„ Downloading replacement datasets for better web analytics...\n")

    success_count = 0
    total_datasets = len(REPLACEMENT_DATASETS)

    for dataset_name, dataset_path in REPLACEMENT_DATASETS.items():
        success = download_kaggle_dataset(dataset_name, dataset_path, dataset_name)
        if success:
            success_count += 1
        print("-" * 30)

    print(f"\nğŸ“Š Replacement Datasets Summary: {success_count}/{total_datasets} downloaded successfully")
    return success_count

def download_github_datasets():
    """Download datasets from GitHub"""
    print("\nğŸ™ Downloading datasets from GitHub...\n")

    success_count = 0

    # Download clothing data (already working)
    if download_github_dataset('clothing_github',
                             'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/Ecdat/Clothing.csv',
                             'clothing_data.csv'):
        success_count += 1

    # Download Fair dataset (economics/consumer behavior)
    if download_github_dataset('consumer_github',
                             'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/Ecdat/Fair.csv',
                             'consumer_data.csv'):
        success_count += 1

    # Download BudgetFood dataset (consumer spending)
    if download_github_dataset('budget_github',
                             'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/Ecdat/BudgetFood.csv',
                             'budget_data.csv'):
        success_count += 1

    print(f"\nğŸ“Š GitHub Datasets Summary: {success_count} downloaded successfully")
    return success_count

def download_huggingface_dataset(dataset_name, hf_dataset_id):
    """Download dataset from Hugging Face"""
    try:
        print(f"ğŸ¤— Downloading {dataset_name} from Hugging Face...")

        from datasets import load_dataset

        # Load dataset
        dataset = load_dataset(hf_dataset_id, split='train')

        # Convert to pandas DataFrame
        df = dataset.to_pandas()

        # Create directory and save
        os.makedirs(f'/content/datasets/{dataset_name}', exist_ok=True)
        df.to_csv(f'/content/datasets/{dataset_name}/{dataset_name}.csv', index=False)

        print(f"âœ… {dataset_name} downloaded from Hugging Face: {df.shape}")
        return True

    except Exception as e:
        print(f"âŒ Failed to download {dataset_name} from Hugging Face: {str(e)}")
        return False

def download_huggingface_datasets():
    """Download datasets from Hugging Face"""
    print("\nğŸ¤— Downloading datasets from Hugging Face...\n")

    success_count = 0

    # Try to download some public datasets with proper configs
    hf_datasets = [
        ('amazon_electronics', 'McAuley-Lab/Amazon-Reviews-2023', 'raw_review_Electronics'),
        ('amazon_beauty', 'McAuley-Lab/Amazon-Reviews-2023', 'raw_review_All_Beauty'),
        ('web_nlp', 'SetFit/20_newsgroups', 'default')
    ]

    for dataset_name, hf_id, config in hf_datasets:
        try:
            if download_huggingface_dataset_with_config(dataset_name, hf_id, config):
                success_count += 1
        except Exception as e:
            print(f"âŒ {dataset_name} failed: {str(e)}")

    print(f"\nğŸ“Š Hugging Face Datasets Summary: {success_count} downloaded successfully")
    return success_count

def download_huggingface_dataset_with_config(dataset_name, hf_dataset_id, config):
    """Download dataset from Hugging Face with specific config"""
    try:
        print(f"ğŸ¤— Downloading {dataset_name} from Hugging Face...")

        from datasets import load_dataset

        # Load dataset with specific config
        dataset = load_dataset(hf_dataset_id, config, split='train', streaming=False)

        # Convert to pandas DataFrame (limit rows for memory)
        df = dataset.to_pandas()
        if len(df) > 10000:
            df = df.head(10000)  # Limit to 10k rows

        # Create directory and save
        os.makedirs(f'/content/datasets/{dataset_name}', exist_ok=True)
        df.to_csv(f'/content/datasets/{dataset_name}/{dataset_name}.csv', index=False)

        print(f"âœ… {dataset_name} downloaded from Hugging Face: {df.shape}")
        return True

    except Exception as e:
        print(f"âŒ Failed to download {dataset_name} from Hugging Face: {str(e)}")
        return False

# =============================================================================
# STEP 5: SMART CSV READING WITH DELIMITER DETECTION
# =============================================================================

def smart_read_csv(file_path, nrows=1000):
    """Intelligently read CSV files with different delimiters"""
    try:
        # First, peek at the first few lines to detect delimiter
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            first_line = f.readline()
            second_line = f.readline()

        # Count potential delimiters in first lines
        delimiters = [',', ';', '\t', '|']
        delimiter_counts = {}

        for delim in delimiters:
            count1 = first_line.count(delim)
            count2 = second_line.count(delim) if second_line else 0
            # Use delimiter if it appears consistently and more than 0 times
            if count1 > 0 and count1 == count2:
                delimiter_counts[delim] = count1

        # Choose delimiter with highest count, default to comma
        best_delimiter = ','
        if delimiter_counts:
            best_delimiter = max(delimiter_counts, key=delimiter_counts.get)

        # Read with detected delimiter
        df = pd.read_csv(file_path, delimiter=best_delimiter, nrows=nrows)

        # If still only one column and contains semicolons, force semicolon delimiter
        if len(df.columns) == 1 and ';' in df.columns[0]:
            df = pd.read_csv(file_path, delimiter=';', nrows=nrows)

        return df

    except Exception as e:
        # Try different encodings
        for encoding in ['utf-8', 'latin-1', 'cp1252']:
            try:
                df = pd.read_csv(file_path, encoding=encoding, nrows=nrows)
                return df
            except:
                continue

        print(f"    âŒ Error reading file: {str(e)}")
        return None

# =============================================================================
# STEP 6: FILE DISCOVERY AND EXPLORATION
# =============================================================================

def discover_all_datasets():
    """Discover all downloaded datasets"""
    print("ğŸ” Discovering all dataset files...\n")

    all_datasets = {**WORKING_DATASETS, **REPLACEMENT_DATASETS}
    file_map = {}

    for dataset_name in all_datasets.keys():
        folder_path = f'/content/datasets/{dataset_name}'
        if os.path.exists(folder_path):
            # Check for different file types
            csv_files = list(Path(folder_path).glob('*.csv'))
            xlsx_files = list(Path(folder_path).glob('*.xlsx'))
            json_files = list(Path(folder_path).glob('*.json'))

            file_map[dataset_name] = {
                'csv': [f.name for f in csv_files],
                'xlsx': [f.name for f in xlsx_files],
                'json': [f.name for f in json_files]
            }

            print(f"ğŸ“ {dataset_name}:")
            if csv_files:
                for file_name in [f.name for f in csv_files]:
                    print(f"  ğŸ“„ {file_name}")
            elif xlsx_files:
                for file_name in [f.name for f in xlsx_files]:
                    print(f"  ğŸ“Š {file_name}")
            elif json_files:
                for file_name in [f.name for f in json_files]:
                    print(f"  ğŸ“‹ {file_name}")
            else:
                print(f"  âŒ No data files found")
            print()

    # Check GitHub datasets
    github_datasets = ['clothing_github', 'consumer_github', 'budget_github']
    for dataset_name in github_datasets:
        folder_path = f'/content/datasets/{dataset_name}'
        if os.path.exists(folder_path):
            csv_files = list(Path(folder_path).glob('*.csv'))
            if csv_files:
                file_map[dataset_name] = {'csv': [f.name for f in csv_files]}
                print(f"ğŸ“ {dataset_name} (GitHub):")
                for file_name in [f.name for f in csv_files]:
                    print(f"  ğŸ“„ {file_name}")
                print()

    # Check Hugging Face datasets
    hf_datasets = ['amazon_electronics', 'amazon_beauty', 'web_nlp']
    for dataset_name in hf_datasets:
        folder_path = f'/content/datasets/{dataset_name}'
        if os.path.exists(folder_path):
            csv_files = list(Path(folder_path).glob('*.csv'))
            if csv_files:
                file_map[dataset_name] = {'csv': [f.name for f in csv_files]}
                print(f"ğŸ“ {dataset_name} (Hugging Face):")
                for file_name in [f.name for f in csv_files]:
                    print(f"  ğŸ“„ {file_name}")
                print()

    return file_map

# =============================================================================
# STEP 7: DATA LOADERS FOR WORKING DATASETS
# =============================================================================

def load_olist_data():
    """Load Olist e-commerce data (VERIFIED WORKING)"""
    base_path = '/content/datasets/olist_ecommerce'

    try:
        orders = pd.read_csv(f'{base_path}/olist_orders_dataset.csv')
        customers = pd.read_csv(f'{base_path}/olist_customers_dataset.csv')
        order_items = pd.read_csv(f'{base_path}/olist_order_items_dataset.csv')
        products = pd.read_csv(f'{base_path}/olist_products_dataset.csv')

        print("âœ… Olist data loaded successfully!")
        print(f"Orders: {orders.shape}, Customers: {customers.shape}")
        print(f"Order Items: {order_items.shape}, Products: {products.shape}")

        return {'orders': orders, 'customers': customers, 'order_items': order_items, 'products': products}

    except Exception as e:
        print(f"âŒ Error loading Olist data: {str(e)}")
        return None

def load_online_retail_data():
    """Load online retail data (Excel format)"""
    try:
        # Try Excel format first
        retail_data = pd.read_excel('/content/datasets/online_retail/online_retail_II.xlsx')
        print(f"âœ… Online retail data loaded: {retail_data.shape}")
        print(f"Columns: {list(retail_data.columns[:5])}{'...' if len(retail_data.columns) > 5 else ''}")
        return retail_data

    except Exception as e:
        # Try CSV format as backup
        try:
            csv_files = list(Path('/content/datasets/online_retail').glob('*.csv'))
            if csv_files:
                retail_data = smart_read_csv(csv_files[0])
                print(f"âœ… Online retail data loaded from CSV: {retail_data.shape}")
                return retail_data
        except:
            pass

        print(f"âŒ Error loading online retail data: {str(e)}")
        return None

def load_ab_testing_data():
    """Load A/B testing data (VERIFIED WORKING)"""
    try:
        control = pd.read_csv('/content/datasets/ab_testing/control_group.csv', delimiter=';')
        test = pd.read_csv('/content/datasets/ab_testing/test_group.csv', delimiter=';')

        print(f"âœ… A/B testing data loaded successfully!")
        print(f"Control group: {control.shape}, Test group: {test.shape}")

        return {'control': control, 'test': test}

    except Exception as e:
        print(f"âŒ Error loading A/B testing data: {str(e)}")
        return None

def load_cookie_cats_data():
    """Load Cookie Cats A/B testing data (VERIFIED WORKING)"""
    try:
        cookie_cats = pd.read_csv('/content/datasets/cookie_cats/cookie_cats.csv')

        print(f"âœ… Cookie Cats data loaded: {cookie_cats.shape}")
        print(f"Columns: {list(cookie_cats.columns)}")

        return cookie_cats

    except Exception as e:
        print(f"âŒ Error loading Cookie Cats data: {str(e)}")
        return None

def load_marketing_campaign_data():
    """Load marketing campaign data (VERIFIED WORKING)"""
    try:
        # Force semicolon delimiter since this data uses semicolons
        marketing = pd.read_csv('/content/datasets/marketing_campaign/marketing_campaign.csv', delimiter=';')

        print(f"âœ… Marketing campaign data loaded: {marketing.shape}")
        print(f"Columns: {list(marketing.columns[:5])}{'...' if len(marketing.columns) > 5 else ''}")

        return marketing

    except Exception as e:
        print(f"âŒ Error loading marketing campaign data: {str(e)}")
        return None

def load_ecommerce_behavior_data():
    """Load e-commerce behavior data (VERIFIED WORKING)"""
    base_path = '/content/datasets/ecommerce_behavior'

    try:
        # Load both monthly files with timeout protection
        print("Loading November 2019 data...")
        nov_data = pd.read_csv(f'{base_path}/2019-Nov.csv', nrows=50000)  # Limit rows to prevent hanging

        print("Loading October 2019 data...")
        oct_data = pd.read_csv(f'{base_path}/2019-Oct.csv', nrows=50000)  # Limit rows to prevent hanging

        print(f"âœ… E-commerce behavior data loaded!")
        print(f"November 2019: {nov_data.shape}, October 2019: {oct_data.shape}")
        print(f"Columns: {list(nov_data.columns)}")

        return {'november': nov_data, 'october': oct_data}

    except Exception as e:
        print(f"âŒ Error loading e-commerce behavior data: {str(e)}")
        return None

def load_amazon_products_data():
    """Load Amazon products data (VERIFIED WORKING)"""
    base_path = '/content/datasets/amazon_products'

    try:
        products = pd.read_csv(f'{base_path}/amazon_products.csv')
        categories = pd.read_csv(f'{base_path}/amazon_categories.csv')

        print(f"âœ… Amazon products data loaded!")
        print(f"Products: {products.shape}, Categories: {categories.shape}")

        return {'products': products, 'categories': categories}

    except Exception as e:
        print(f"âŒ Error loading Amazon products data: {str(e)}")
        return None

def load_retail_analytics_data():
    """Load retail analytics data (VERIFIED WORKING)"""
    base_path = '/content/datasets/retail_analytics'

    try:
        sales = smart_read_csv(f'{base_path}/sales data-set.csv')
        stores = smart_read_csv(f'{base_path}/stores data-set.csv')
        features = smart_read_csv(f'{base_path}/Features data set.csv')

        print(f"âœ… Retail analytics data loaded!")
        print(f"Sales: {sales.shape}, Stores: {stores.shape}, Features: {features.shape}")

        return {'sales': sales, 'stores': stores, 'features': features}

    except Exception as e:
        print(f"âŒ Error loading retail analytics data: {str(e)}")
        return None

# =============================================================================
# STEP 8: REPLACEMENT DATASET LOADERS
# =============================================================================

def load_digital_ads_data():
    """Load digital ads conversion data (WORKING)"""
    try:
        ads_data = pd.read_csv('/content/datasets/digital_ads_real/KAG_conversion_data.csv')
        print(f"âœ… Digital ads data loaded: {ads_data.shape}")
        print(f"Columns: {list(ads_data.columns)}")
        return ads_data
    except Exception as e:
        print(f"âŒ Error loading digital ads data: {str(e)}")
        return None

def load_ecommerce_events_data():
    """Load e-commerce events data (WORKING)"""
    base_path = '/content/datasets/ecommerce_events'

    try:
        # Load available monthly files
        available_files = list(Path(base_path).glob('*.csv'))
        datasets = {}

        for file in available_files[:3]:  # Load first 3 files to avoid memory issues
            month_name = file.stem
            df = pd.read_csv(file, nrows=25000)  # Limit rows
            datasets[month_name] = df
            print(f"âœ… Loaded {month_name}: {df.shape}")

        print(f"âœ… E-commerce events data loaded: {len(datasets)} files")
        return datasets

    except Exception as e:
        print(f"âŒ Error loading e-commerce events data: {str(e)}")
        return None

def load_replacement_datasets():
    """Load any available replacement datasets"""
    datasets_loaded = {}

    # Try customer shopping dataset
    try:
        shopping_files = list(Path('/content/datasets/customer_shopping').glob('*.csv'))
        if shopping_files:
            shopping_data = smart_read_csv(shopping_files[0])
            datasets_loaded['customer_shopping'] = shopping_data
            print(f"âœ… Customer shopping data loaded: {shopping_data.shape}")
    except Exception as e:
        print(f"âŒ Failed to load customer shopping: {str(e)}")

    # Try e-commerce sales dataset
    try:
        sales_files = list(Path('/content/datasets/ecommerce_sales').glob('*.csv'))
        if sales_files:
            sales_data = smart_read_csv(sales_files[0])
            datasets_loaded['ecommerce_sales'] = sales_data
            print(f"âœ… E-commerce sales data loaded: {sales_data.shape}")
    except Exception as e:
        print(f"âŒ Failed to load e-commerce sales: {str(e)}")

    # Try online sales dataset
    try:
        online_files = list(Path('/content/datasets/online_sales').glob('*.csv'))
        if online_files:
            online_data = smart_read_csv(online_files[0])
            datasets_loaded['online_sales'] = online_data
            print(f"âœ… Online sales data loaded: {online_data.shape}")
    except Exception as e:
        print(f"âŒ Failed to load online sales: {str(e)}")

    # Try Facebook ads dataset
    try:
        fb_files = list(Path('/content/datasets/facebook_ads').glob('*.csv'))
        if fb_files:
            fb_data = smart_read_csv(fb_files[0])
            datasets_loaded['facebook_ads'] = fb_data
            print(f"âœ… Facebook ads data loaded: {fb_data.shape}")
    except Exception as e:
        print(f"âŒ Failed to load Facebook ads: {str(e)}")

    # Try advertising dataset
    try:
        ad_files = list(Path('/content/datasets/advertising_data').glob('*.csv'))
        if ad_files:
            ad_data = smart_read_csv(ad_files[0])
            datasets_loaded['advertising'] = ad_data
            print(f"âœ… Advertising data loaded: {ad_data.shape}")
    except Exception as e:
        print(f"âŒ Failed to load advertising data: {str(e)}")

    return datasets_loaded if datasets_loaded else None

def download_additional_backups():
    """Download additional backup datasets if main ones fail"""
    print("\nğŸ”„ Downloading additional backup datasets...\n")

    backup_datasets = {
        'superstore_sales': 'bravehart101/sample-superstore-dataset',
        'website_analytics': 'berkayalan/online-sales-dataset',
        'customer_behavior': 'iamsouravbanerjee/customer-shopping-trends-dataset',
        'ecommerce_simple': 'carrie1/ecommerce-data',
        'digital_marketing_simple': 'fayomi/advertising-prediction-dataset'
    }

    success_count = 0
    for dataset_name, dataset_path in backup_datasets.items():
        if download_kaggle_dataset(dataset_name, dataset_path, dataset_name):
            success_count += 1
        print("-" * 30)

    print(f"\nğŸ“Š Backup Datasets Summary: {success_count}/{len(backup_datasets)} downloaded successfully")
    return success_count

def load_backup_datasets():
    """Load backup datasets"""
    datasets_loaded = {}

    backup_folders = ['superstore_sales', 'website_analytics', 'customer_behavior',
                     'ecommerce_simple', 'digital_marketing_simple']

    for folder in backup_folders:
        try:
            csv_files = list(Path(f'/content/datasets/{folder}').glob('*.csv'))
            if csv_files:
                backup_data = smart_read_csv(csv_files[0])
                datasets_loaded[folder] = backup_data
                print(f"âœ… Backup {folder} loaded: {backup_data.shape}")
        except Exception as e:
            print(f"âŒ Error loading backup {folder}: {str(e)}")

    return datasets_loaded if datasets_loaded else None

def load_github_datasets():
    """Load GitHub datasets"""
    datasets_loaded = {}

    # Try clothing data
    try:
        clothing_data = pd.read_csv('/content/datasets/clothing_github/clothing_data.csv')
        datasets_loaded['clothing'] = clothing_data
        print(f"âœ… GitHub clothing data loaded: {clothing_data.shape}")
    except Exception as e:
        print(f"âŒ Error loading GitHub clothing data: {str(e)}")

    # Try consumer data
    try:
        consumer_data = pd.read_csv('/content/datasets/consumer_github/consumer_data.csv')
        datasets_loaded['consumer_behavior'] = consumer_data
        print(f"âœ… GitHub consumer data loaded: {consumer_data.shape}")
    except Exception as e:
        print(f"âŒ Error loading GitHub consumer data: {str(e)}")

    # Try budget data
    try:
        budget_data = pd.read_csv('/content/datasets/budget_github/budget_data.csv')
        datasets_loaded['budget_analysis'] = budget_data
        print(f"âœ… GitHub budget data loaded: {budget_data.shape}")
    except Exception as e:
        print(f"âŒ Error loading GitHub budget data: {str(e)}")

    return datasets_loaded if datasets_loaded else None

def load_huggingface_datasets():
    """Load Hugging Face datasets"""
    datasets_loaded = {}

    # Try to load any HF datasets that were downloaded
    hf_folders = ['amazon_electronics', 'amazon_beauty', 'web_nlp']

    for folder in hf_folders:
        try:
            csv_files = list(Path(f'/content/datasets/{folder}').glob('*.csv'))
            if csv_files:
                hf_data = smart_read_csv(csv_files[0], nrows=5000)  # Limit rows for memory
                datasets_loaded[folder] = hf_data
                print(f"âœ… Hugging Face {folder} loaded: {hf_data.shape}")
        except Exception as e:
            print(f"âŒ Error loading HF {folder}: {str(e)}")

    return datasets_loaded if datasets_loaded else None

# =============================================================================
# STEP 9: COMPREHENSIVE TESTING
# =============================================================================

def test_all_real_datasets():
    """Test all real dataset loaders"""
    print("ğŸ§ª Testing all REAL dataset loaders...\n")

    results = {}

    # Test verified working datasets
    print("1ï¸âƒ£ Testing Olist E-commerce data:")
    results['olist'] = load_olist_data()
    print()

    print("2ï¸âƒ£ Testing Online Retail data:")
    results['online_retail'] = load_online_retail_data()
    print()

    print("3ï¸âƒ£ Testing A/B testing data:")
    results['ab_testing'] = load_ab_testing_data()
    print()

    print("4ï¸âƒ£ Testing Cookie Cats data:")
    results['cookie_cats'] = load_cookie_cats_data()
    print()

    print("5ï¸âƒ£ Testing Marketing Campaign data:")
    results['marketing'] = load_marketing_campaign_data()
    print()

    print("6ï¸âƒ£ Testing E-commerce Behavior data:")
    try:
        results['ecommerce_behavior'] = load_ecommerce_behavior_data()
    except Exception as e:
        print(f"âŒ Error in e-commerce behavior: {str(e)}")
        results['ecommerce_behavior'] = None
    print()

    print("7ï¸âƒ£ Testing Amazon Products data:")
    results['amazon'] = load_amazon_products_data()
    print()

    print("8ï¸âƒ£ Testing Retail Analytics data:")
    results['retail'] = load_retail_analytics_data()
    print()

    # Test replacement datasets that have files
    print("9ï¸âƒ£ Testing Digital Ads data:")
    results['digital_ads'] = load_digital_ads_data()
    print()

    print("ğŸ”Ÿ Testing E-commerce Events data:")
    results['ecommerce_events'] = load_ecommerce_events_data()
    print()

    # Test additional replacement datasets
    print("1ï¸âƒ£1ï¸âƒ£ Testing Replacement datasets:")
    results['replacements'] = load_replacement_datasets()
    print()

    # Test GitHub datasets
    print("1ï¸âƒ£2ï¸âƒ£ Testing GitHub datasets:")
    results['github'] = load_github_datasets()
    print()

    # Test Hugging Face datasets
    print("1ï¸âƒ£3ï¸âƒ£ Testing Hugging Face datasets:")
    try:
        results['huggingface'] = load_huggingface_datasets()
    except Exception as e:
        print(f"âŒ Hugging Face loading failed: {str(e)}")
        results['huggingface'] = None
    print()

    # Summary
    successful_loads = sum(1 for v in results.values() if v is not None)
    total_tests = len(results)

    print(f"âœ¨ Testing complete! {successful_loads}/{total_tests} dataset groups loaded successfully")

    # Detailed summary
    working_datasets = []
    for key, value in results.items():
        if value is not None:
            if isinstance(value, dict):
                working_datasets.append(f"{key} ({len(value)} files)")
            else:
                working_datasets.append(key)

    print(f"\nğŸ¯ Successfully loaded datasets:")
    for dataset in working_datasets:
        print(f"   âœ… {dataset}")

    return results

# =============================================================================
# STEP 10: MAIN EXECUTION
# =============================================================================

def main():
    """Main function to setup and download all REAL datasets"""
    print("ğŸ¯ WEB ANALYTICS & OPTIMIZATION PROJECT")
    print("ğŸ“¦ Real Dataset Downloader for Google Colab")
    print("ğŸš« NO SYNTHETIC DATA - REAL DATASETS ONLY")
    print("=" * 60)

    # Step 1: Setup Kaggle API
    setup_kaggle_api()
    print()

    # Step 2: Install packages
    install_packages()
    print()

    # Step 3: Download verified working datasets
    working_count = download_all_working_datasets()

    # Step 4: Download replacement datasets for better coverage
    replacement_count = download_replacement_datasets()

    # Step 5: Download GitHub datasets
    github_count = download_github_datasets()

    # Step 6: Try downloading from Hugging Face
    try:
        hf_count = download_huggingface_datasets()
    except Exception as e:
        print(f"âš ï¸ Hugging Face downloads failed: {str(e)}")
        hf_count = 0

    # Step 7: Discover all files
    file_map = discover_all_datasets()

    print("ğŸ‰ Download phase complete!")
    print(f"ğŸ“Š Summary:")
    print(f"   Working datasets: {working_count}")
    print(f"   Replacement datasets: {replacement_count}")
    print(f"   GitHub datasets: {github_count}")
    print(f"   Hugging Face datasets: {hf_count}")
    print(f"   Total real datasets available: {working_count + replacement_count + github_count + hf_count}")

    print("\nğŸ“ Next steps:")
    print("1. Test dataset loading")
    print("2. Choose the best datasets for your analysis")
    print("3. Start with data cleaning and integration")
    print("4. Build your analytics models")

    return file_map

# =============================================================================
# EXECUTION
# =============================================================================

if __name__ == "__main__":
    try:
        # Run the main setup
        file_map = main()

        # Test all real datasets
        print("\n" + "="*60)
        results = test_all_real_datasets()

        # Count successful datasets
        successful_datasets = [k for k, v in results.items() if v is not None]

        # If we don't have enough datasets, download backups
        if len(successful_datasets) < 8:
            print(f"\nğŸ”„ Only {len(successful_datasets)} dataset groups loaded. Downloading backup datasets...")
            backup_count = download_additional_backups()

            if backup_count > 0:
                print("\nğŸ§ª Testing backup datasets...")
                backup_results = load_backup_datasets()
                if backup_results:
                    results['backups'] = backup_results
                    successful_datasets.append('backups')

        # Final summary
        print(f"\nğŸ¯ FINAL SUMMARY - REAL DATASETS ONLY:")
        print(f"âœ… Successfully loaded dataset groups: {len(successful_datasets)}")

        # Count total individual datasets
        total_individual_datasets = 0
        for k, v in results.items():
            if v is not None:
                if isinstance(v, dict):
                    total_individual_datasets += len(v)
                else:
                    total_individual_datasets += 1

        print(f"ğŸ“Š Total individual datasets: {total_individual_datasets}")
        print(f"ğŸ“‹ Available dataset groups: {', '.join(successful_datasets)}")

        if len(successful_datasets) >= 8:
            print("\nğŸš€ EXCELLENT! You have a comprehensive collection of real datasets for web analytics!")
        elif len(successful_datasets) >= 5:
            print("\nâœ… GOOD! You have sufficient real datasets to start your analysis!")
        elif len(successful_datasets) >= 3:
            print("\nâš ï¸ LIMITED but workable. You have basic datasets for analysis.")
        else:
            print("\nâŒ FEW datasets available. Consider checking internet connection or trying individual downloads.")

        print("\nğŸ‰ Ready to build your web analytics platform with REAL data!")
        print("\nğŸ’¡ Recommended starting datasets:")
        if results.get('olist') is not None: print("   ğŸ”¹ Olist E-commerce (comprehensive)")
        if results.get('online_retail') is not None: print("   ğŸ”¹ Online Retail (transaction analysis)")
        if results.get('ab_testing') is not None: print("   ğŸ”¹ A/B Testing (conversion optimization)")
        if results.get('digital_ads') is not None: print("   ğŸ”¹ Digital Ads (marketing analytics)")
        if results.get('ecommerce_behavior') is not None: print("   ğŸ”¹ E-commerce Behavior (user tracking)")

    except Exception as e:
        print(f"\nâŒ Setup failed with error: {str(e)}")
        print("ğŸ’¡ Trying to download backup datasets as fallback...")
        try:
            backup_count = download_additional_backups()
            if backup_count > 0:
                backup_results = load_backup_datasets()
                print(f"\nâœ… Loaded {len(backup_results) if backup_results else 0} backup datasets!")
            else:
                print("\nâŒ Backup downloads also failed. Please check:")
                print("   - Internet connection")
                print("   - Kaggle API credentials")
                print("   - Try running individual dataset downloads")
        except Exception as backup_error:
            print(f"âŒ Backup downloads failed: {str(backup_error)}")
            print("ğŸ”§ Please try manual dataset downloads or check your setup.")

"""# A/B TESTING FRAMEWORK - CORE SETUP
# Advanced statistical methods for A/B testing

"""

# A/B TESTING FRAMEWORK - CORE SETUP
# Advanced statistical methods for A/B testing

import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy.stats import norm, beta, gamma, chi2_contingency
from statsmodels.stats.power import ttest_power, zt_ind_solve_power
from statsmodels.stats.proportion import proportions_ztest, proportion_effectsize
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import warnings
import math
from datetime import datetime, timedelta
from pathlib import Path

warnings.filterwarnings('ignore')

class AdvancedABTesting:
    """
    Enhanced A/B Testing Framework specifically designed for your real datasets:
    - A/B Testing Data: control_group.csv + test_group.csv (Facebook ads)
    - Cookie Cats: Mobile game A/B test with 90K users
    - Digital Ads: Conversion tracking with 1143 records

    Features:
    - Power analysis and sample size calculation
    - Sequential testing (early stopping)
    - Bayesian analysis with Beta-Binomial conjugate priors
    - Multiple testing corrections
    - Effect size calculations
    - Real-time monitoring capabilities
    """

    def __init__(self, data_path='/content/datasets'):
        """Initialize with your dataset paths"""
        self.data_path = data_path
        self.test_results = {}
        self.sequential_results = []
        self.datasets = {}

        print("ğŸš€ ENHANCED A/B TESTING FRAMEWORK INITIALIZED")
        print("=" * 55)
        print("ğŸ¯ Designed for Your Real Datasets:")
        print("   ğŸ“Š A/B Testing Data (Facebook Ads)")
        print("   ğŸ® Cookie Cats (Mobile Game Retention)")
        print("   ğŸ’° Digital Ads (Conversion Tracking)")
        print("=" * 55)

    def load_your_datasets(self):
        """Load your specific A/B testing datasets"""
        try:
            # Load A/B Testing Data (Facebook Ads Campaign)
            control_path = f'{self.data_path}/ab_testing/control_group.csv'
            test_path = f'{self.data_path}/ab_testing/test_group.csv'

            if Path(control_path).exists() and Path(test_path).exists():
                self.datasets['ab_facebook'] = {
                    'control': pd.read_csv(control_path, delimiter=';'),
                    'test': pd.read_csv(test_path, delimiter=';')
                }
                print("âœ… Facebook Ads A/B Test Data Loaded")
                print(f"   Control: {self.datasets['ab_facebook']['control'].shape}")
                print(f"   Test: {self.datasets['ab_facebook']['test'].shape}")

            # Load Cookie Cats Data (Mobile Game A/B Test)
            cookie_path = f'{self.data_path}/cookie_cats/cookie_cats.csv'
            if Path(cookie_path).exists():
                self.datasets['cookie_cats'] = pd.read_csv(cookie_path)
                print("âœ… Cookie Cats Mobile Game A/B Test Loaded")
                print(f"   Total Users: {self.datasets['cookie_cats'].shape}")
                print(f"   Versions: {self.datasets['cookie_cats']['version'].unique()}")

            # Load Digital Ads Conversion Data
            digital_ads_path = f'{self.data_path}/digital_ads_real/KAG_conversion_data.csv'
            if Path(digital_ads_path).exists():
                self.datasets['digital_ads'] = pd.read_csv(digital_ads_path)
                print("âœ… Digital Ads Conversion Data Loaded")
                print(f"   Records: {self.datasets['digital_ads'].shape}")
                print(f"   Campaigns: {self.datasets['digital_ads']['xyz_campaign_id'].nunique()}")

            # Summary
            print(f"\nğŸ“Š DATASET SUMMARY:")
            print(f"   Total A/B Test Datasets: {len(self.datasets)}")
            for name, data in self.datasets.items():
                if isinstance(data, dict):
                    total_records = sum(df.shape[0] for df in data.values())
                    print(f"   {name}: {total_records:,} total records")
                else:
                    print(f"   {name}: {data.shape[0]:,} records")

            return True

        except Exception as e:
            print(f"âŒ Error loading datasets: {str(e)}")
            return False

    def inspect_dataset_structure(self, dataset_name=None):
        """Inspect the structure of your loaded datasets"""
        if not self.datasets:
            print("âŒ No datasets loaded. Run load_your_datasets() first.")
            return

        datasets_to_inspect = [dataset_name] if dataset_name else list(self.datasets.keys())

        for name in datasets_to_inspect:
            if name not in self.datasets:
                print(f"âŒ Dataset '{name}' not found")
                continue

            print(f"\nğŸ” INSPECTING DATASET: {name.upper()}")
            print("=" * 50)

            data = self.datasets[name]

            if name == 'ab_facebook':
                # Facebook Ads A/B Test Analysis
                control_df = data['control']
                test_df = data['test']

                print("ğŸ“Š FACEBOOK ADS A/B TEST STRUCTURE:")
                print(f"Control Group Columns: {list(control_df.columns)}")
                print(f"Test Group Columns: {list(test_df.columns)}")

                # Show key metrics
                print(f"\nğŸ“ˆ KEY METRICS AVAILABLE:")
                if '# of Purchase' in control_df.columns:
                    print(f"   ğŸ’° Purchases (Control): {control_df['# of Purchase'].sum()}")
                    print(f"   ğŸ’° Purchases (Test): {test_df['# of Purchase'].sum()}")

                if 'Spent' in control_df.columns:
                    print(f"   ğŸ’¸ Total Spend (Control): ${control_df['Spent'].sum():.2f}")
                    print(f"   ğŸ’¸ Total Spend (Test): ${test_df['Spent'].sum():.2f}")

                if '# of Website Clicks' in control_df.columns:
                    print(f"   ğŸ–±ï¸ Clicks (Control): {control_df['# of Website Clicks'].sum()}")
                    print(f"   ğŸ–±ï¸ Clicks (Test): {test_df['# of Website Clicks'].sum()}")

            elif name == 'cookie_cats':
                # Cookie Cats Mobile Game Analysis
                print("ğŸ® COOKIE CATS MOBILE GAME STRUCTURE:")
                print(f"Columns: {list(data.columns)}")
                print(f"Total Users: {len(data):,}")

                # Version distribution
                version_dist = data['version'].value_counts()
                print(f"\nğŸ“± VERSION DISTRIBUTION:")
                for version, count in version_dist.items():
                    print(f"   {version}: {count:,} users ({count/len(data)*100:.1f}%)")

                # Retention metrics
                if 'retention_1' in data.columns and 'retention_7' in data.columns:
                    retention_1 = data.groupby('version')['retention_1'].mean()
                    retention_7 = data.groupby('version')['retention_7'].mean()

                    print(f"\nğŸ“ˆ RETENTION RATES BY VERSION:")
                    for version in data['version'].unique():
                        print(f"   {version}:")
                        print(f"     1-day retention: {retention_1[version]*100:.2f}%")
                        print(f"     7-day retention: {retention_7[version]*100:.2f}%")

            elif name == 'digital_ads':
                # Digital Ads Conversion Analysis
                print("ğŸ’° DIGITAL ADS CONVERSION STRUCTURE:")
                print(f"Columns: {list(data.columns)}")
                print(f"Total Records: {len(data):,}")

                # Campaign summary
                if 'xyz_campaign_id' in data.columns:
                    campaigns = data['xyz_campaign_id'].nunique()
                    print(f"   ğŸ“Š Unique Campaigns: {campaigns}")

                # Conversion metrics
                if 'Total_Conversion' in data.columns and 'Spent' in data.columns:
                    total_conversions = data['Total_Conversion'].sum()
                    total_spend = data['Spent'].sum()
                    avg_cost_per_conversion = total_spend / total_conversions if total_conversions > 0 else 0

                    print(f"\nğŸ’¸ OVERALL PERFORMANCE:")
                    print(f"   Total Conversions: {total_conversions:,}")
                    print(f"   Total Spend: ${total_spend:,.2f}")
                    print(f"   Avg Cost per Conversion: ${avg_cost_per_conversion:.2f}")

                # Demographic breakdown
                if 'age' in data.columns and 'gender' in data.columns:
                    print(f"\nğŸ‘¥ DEMOGRAPHIC BREAKDOWN:")
                    demo_performance = data.groupby(['age', 'gender']).agg({
                        'Total_Conversion': 'sum',
                        'Spent': 'sum'
                    }).reset_index()
                    demo_performance['cost_per_conversion'] = (
                        demo_performance['Spent'] / demo_performance['Total_Conversion']
                    ).round(2)

                    best_performing = demo_performance.loc[
                        demo_performance['cost_per_conversion'].idxmin()
                    ]
                    print(f"   Best Performing: {best_performing['age']} {best_performing['gender']}")
                    print(f"   Cost per Conversion: ${best_performing['cost_per_conversion']:.2f}")

# Initialize the framework with your data
def initialize_ab_framework():
    """Initialize the A/B testing framework with your datasets"""

    # Create framework instance
    ab_tester = AdvancedABTesting()

    # Load your datasets
    if ab_tester.load_your_datasets():
        print("\nğŸ‰ SUCCESS! Your A/B testing datasets are ready for analysis!")

        # Quick inspection of all datasets
        ab_tester.inspect_dataset_structure()

        return ab_tester
    else:
        print("\nâŒ Failed to load datasets. Please check your data paths.")
        return None

# Usage example
if __name__ == "__main__":
    print("ğŸ§ª ENHANCED A/B TESTING FRAMEWORK - CHUNK 1")
    print("Ready to initialize with your real datasets!")
    print("\nUsage:")
    print("ab_tester = initialize_ab_framework()")
    print("ab_tester.inspect_dataset_structure('cookie_cats')")  # Inspect specific dataset

ab_tester = initialize_ab_framework()

ab_tester.inspect_dataset_structure('cookie_cats')

"""## Sample Size Calculation & Power Analysis"""

# First, initialize your ab_tester (from Chunk 1)
ab_tester = initialize_ab_framework()

# DIRECT IMPLEMENTATION (NO IMPORT NEEDED)
# Run this code directly in your notebook after initializing ab_tester

import pandas as pd
import numpy as np
from scipy.stats import norm
from statsmodels.stats.power import zt_ind_solve_power
from statsmodels.stats.proportion import proportions_ztest, proportion_effectsize

def inspect_dataset_columns(ab_tester, dataset_name):
    """Inspect actual column names in your datasets"""
    print(f"ğŸ” INSPECTING ACTUAL COLUMNS IN {dataset_name.upper()}")
    print("=" * 50)

    if dataset_name not in ab_tester.datasets:
        print(f"âŒ Dataset '{dataset_name}' not loaded")
        return None

    if dataset_name == 'cookie_cats':
        data = ab_tester.datasets['cookie_cats']
        print(f"ğŸ“Š Cookie Cats columns: {list(data.columns)}")
        print(f"ğŸ“Š Data shape: {data.shape}")
        print(f"ğŸ“Š Sample data:\n{data.head()}")
        return list(data.columns)

    elif dataset_name == 'ab_facebook':
        control_data = ab_tester.datasets['ab_facebook']['control']
        test_data = ab_tester.datasets['ab_facebook']['test']
        print(f"ğŸ“Š Facebook Control columns: {list(control_data.columns)}")
        print(f"ğŸ“Š Facebook Test columns: {list(test_data.columns)}")
        print(f"ğŸ“Š Control shape: {control_data.shape}")
        print(f"ğŸ“Š Test shape: {test_data.shape}")
        print(f"ğŸ“Š Control sample:\n{control_data.head()}")
        return list(control_data.columns)

    elif dataset_name == 'digital_ads':
        data = ab_tester.datasets['digital_ads']
        print(f"ğŸ“Š Digital Ads columns: {list(data.columns)}")
        print(f"ğŸ“Š Data shape: {data.shape}")
        print(f"ğŸ“Š Sample data:\n{data.head()}")
        return list(data.columns)

    return None

def get_baseline_rate_fixed(ab_tester, dataset_name, metric):
    """Extract baseline conversion rate with robust column handling"""

    if dataset_name == 'cookie_cats':
        data = ab_tester.datasets['cookie_cats']

        if metric == 'retention_1':
            if 'retention_1' in data.columns:
                return data['retention_1'].mean()
            else:
                print(f"âŒ Column 'retention_1' not found in cookie_cats")
                return None
        elif metric == 'retention_7':
            if 'retention_7' in data.columns:
                return data['retention_7'].mean()
            else:
                print(f"âŒ Column 'retention_7' not found in cookie_cats")
                return None

    elif dataset_name == 'ab_facebook':
        control_data = ab_tester.datasets['ab_facebook']['control']

        # Check for different possible column names
        purchase_columns = ['# of Purchase', 'purchases', 'Purchase', 'conversions']
        click_columns = ['# of Website Clicks', 'clicks', 'Clicks', 'website_clicks']
        impression_columns = ['# of Impressions', 'impressions', 'Impressions', 'reach']

        # Find actual column names
        actual_purchase_col = None
        actual_click_col = None
        actual_impression_col = None

        for col in purchase_columns:
            if col in control_data.columns:
                actual_purchase_col = col
                break

        for col in click_columns:
            if col in control_data.columns:
                actual_click_col = col
                break

        for col in impression_columns:
            if col in control_data.columns:
                actual_impression_col = col
                break

        print(f"ğŸ“Š Found columns - Purchase: {actual_purchase_col}, Clicks: {actual_click_col}, Impressions: {actual_impression_col}")

        if metric == 'purchase_rate':
            if actual_purchase_col and actual_impression_col:
                total_purchases = control_data[actual_purchase_col].sum()
                total_impressions = control_data[actual_impression_col].sum()
                return total_purchases / total_impressions if total_impressions > 0 else 0
            else:
                print(f"âŒ Required columns not found for purchase_rate")
                return None

        elif metric == 'click_rate':
            if actual_click_col and actual_impression_col:
                total_clicks = control_data[actual_click_col].sum()
                total_impressions = control_data[actual_impression_col].sum()
                return total_clicks / total_impressions if total_impressions > 0 else 0
            else:
                print(f"âŒ Required columns not found for click_rate")
                return None

    elif dataset_name == 'digital_ads':
        data = ab_tester.datasets['digital_ads']

        # Check for different possible column names
        conversion_columns = ['Total_Conversion', 'conversions', 'Conversions', 'total_conversions']
        click_columns = ['Clicks', 'clicks', 'total_clicks']
        impression_columns = ['Impressions', 'impressions', 'total_impressions']

        actual_conversion_col = None
        actual_click_col = None
        actual_impression_col = None

        for col in conversion_columns:
            if col in data.columns:
                actual_conversion_col = col
                break

        for col in click_columns:
            if col in data.columns:
                actual_click_col = col
                break

        for col in impression_columns:
            if col in data.columns:
                actual_impression_col = col
                break

        if metric == 'conversion_rate':
            if actual_conversion_col and actual_impression_col:
                total_conversions = data[actual_conversion_col].sum()
                total_impressions = data[actual_impression_col].sum()
                return total_conversions / total_impressions if total_impressions > 0 else 0
            else:
                print(f"âŒ Required columns not found for conversion_rate")
                return None

        elif metric == 'click_rate':
            if actual_click_col and actual_impression_col:
                total_clicks = data[actual_click_col].sum()
                total_impressions = data[actual_impression_col].sum()
                return total_clicks / total_impressions if total_impressions > 0 else 0
            else:
                print(f"âŒ Required columns not found for click_rate")
                return None

    return None

def calculate_potential_impact_fixed(ab_tester, dataset_name, metric, mde):
    """Calculate potential business impact with robust column handling"""

    impact = {}

    try:
        if dataset_name == 'cookie_cats':
            total_users = len(ab_tester.datasets['cookie_cats'])
            baseline_rate = get_baseline_rate_fixed(ab_tester, dataset_name, metric)

            if baseline_rate:
                current_retaining_users = total_users * baseline_rate
                improved_retaining_users = total_users * baseline_rate * (1 + mde)
                additional_users = improved_retaining_users - current_retaining_users

                impact['Additional Retained Users'] = f"{additional_users:,.0f} users"
                impact['Revenue Impact (est. $5/user)'] = f"${additional_users * 5:,.0f}"

        elif dataset_name == 'ab_facebook':
            control_data = ab_tester.datasets['ab_facebook']['control']

            # Find spend column with multiple possible names
            spend_columns = ['Spent', 'spend', 'cost', 'Cost', 'amount_spent']
            purchase_columns = ['# of Purchase', 'purchases', 'Purchase', 'conversions']

            actual_spend_col = None
            actual_purchase_col = None

            for col in spend_columns:
                if col in control_data.columns:
                    actual_spend_col = col
                    break

            for col in purchase_columns:
                if col in control_data.columns:
                    actual_purchase_col = col
                    break

            print(f"ğŸ“Š Found Facebook columns - Spend: {actual_spend_col}, Purchase: {actual_purchase_col}")

            if actual_purchase_col:
                total_purchases = control_data[actual_purchase_col].sum()
                improved_purchases = total_purchases * (1 + mde)
                additional_purchases = improved_purchases - total_purchases

                impact['Additional Purchases'] = f"{additional_purchases:.0f} purchases"

                if actual_spend_col:
                    total_spend = control_data[actual_spend_col].sum()
                    current_cpa = total_spend / total_purchases if total_purchases > 0 else 0
                    impact['Reduced CPA'] = f"${current_cpa/(1+mde):.2f} (vs ${current_cpa:.2f})"
                else:
                    impact['Note'] = "Spend column not found - using purchase count only"
            else:
                impact['Estimated Impact'] = f"Improved performance by {mde*100:.1f}%"

        elif dataset_name == 'digital_ads':
            data = ab_tester.datasets['digital_ads']

            # Find relevant columns
            conversion_columns = ['Total_Conversion', 'conversions', 'Conversions']

            actual_conversion_col = None

            for col in conversion_columns:
                if col in data.columns:
                    actual_conversion_col = col
                    break

            if actual_conversion_col:
                total_conversions = data[actual_conversion_col].sum()
                improved_conversions = total_conversions * (1 + mde)
                additional_conversions = improved_conversions - total_conversions
                avg_revenue_per_conversion = 50  # Estimate

                impact['Additional Conversions'] = f"{additional_conversions:.0f} conversions"
                impact['Additional Revenue (est.)'] = f"${additional_conversions * avg_revenue_per_conversion:,.0f}"

    except Exception as e:
        print(f"âš ï¸ Warning: Could not calculate impact for {dataset_name}: {str(e)}")
        impact['Note'] = f"Impact calculation needs column verification for {dataset_name}"

    return impact

def calculate_sample_size_for_your_data_fixed(ab_tester, dataset_name='cookie_cats',
                                            metric='retention_1', mde=0.15,
                                            power=0.8, alpha=0.05):
    """
    Calculate required sample size based on YOUR actual dataset baselines
    FIXED to handle actual column names
    """
    print("ğŸ§® CALCULATING SAMPLE SIZE USING YOUR REAL DATA")
    print("=" * 55)

    if dataset_name not in ab_tester.datasets:
        print(f"âŒ Dataset '{dataset_name}' not loaded")
        return None

    # First inspect the dataset to see actual columns
    print("ğŸ” Checking dataset structure...")
    inspect_dataset_columns(ab_tester, dataset_name)

    # Calculate baseline rate from your actual data
    baseline_rate = get_baseline_rate_fixed(ab_tester, dataset_name, metric)

    if baseline_rate is None:
        print(f"âŒ Could not calculate baseline for {metric} in {dataset_name}")
        print("ğŸ’¡ Try inspecting the dataset first to see available columns")
        return None

    print(f"\nğŸ“Š USING YOUR REAL DATA:")
    print(f"   Dataset: {dataset_name}")
    print(f"   Metric: {metric}")
    print(f"   Current Baseline Rate: {baseline_rate*100:.2f}%")

    # Calculate effect size
    new_rate = baseline_rate * (1 + mde)
    effect_size = proportion_effectsize(baseline_rate, new_rate)

    # Calculate required sample size per group
    try:
        sample_size = zt_ind_solve_power(
            effect_size=effect_size,
            power=power,
            alpha=alpha,
            alternative='two-sided'
        )
    except Exception as e:
        # Fallback calculation
        print(f"âš ï¸ Using fallback calculation: {str(e)}")
        z_alpha = norm.ppf(1 - alpha/2)
        z_beta = norm.ppf(power)
        p_pooled = (baseline_rate + new_rate) / 2

        sample_size = (
            2 * p_pooled * (1 - p_pooled) * (z_alpha + z_beta)**2
        ) / (baseline_rate - new_rate)**2

    total_sample_size = sample_size * 2  # Both groups

    # Calculate test duration estimates based on your data volume
    if dataset_name == 'cookie_cats':
        daily_traffic = len(ab_tester.datasets['cookie_cats']) / 30  # Assume 30 days
    else:
        daily_traffic = 1000  # Default

    traffic_scenarios = {
        'current_pace': daily_traffic,
        'conservative_10pct': daily_traffic * 0.1,
        'aggressive_50pct': daily_traffic * 0.5,
        'maximum_100pct': daily_traffic
    }

    duration_estimates = {}
    for scenario, daily_visitors in traffic_scenarios.items():
        if daily_visitors > 0:
            days_needed = total_sample_size / daily_visitors
            duration_estimates[scenario] = days_needed

    # Calculate expected outcomes
    expected_improvement = mde * 100
    potential_impact = calculate_potential_impact_fixed(ab_tester, dataset_name, metric, mde)

    results = {
        'dataset_used': dataset_name,
        'metric_analyzed': metric,
        'baseline_conversion_rate': baseline_rate * 100,
        'target_conversion_rate': new_rate * 100,
        'minimum_detectable_effect': mde * 100,
        'statistical_power': power * 100,
        'significance_level': alpha * 100,
        'effect_size_cohens_h': effect_size,
        'sample_size_per_group': int(sample_size),
        'total_sample_size': int(total_sample_size),
        'duration_estimates': duration_estimates,
        'expected_improvement': expected_improvement,
        'potential_business_impact': potential_impact
    }

    # Display detailed results
    print(f"\nğŸ“ˆ SAMPLE SIZE ANALYSIS RESULTS:")
    print(f"   ğŸ¯ Current Rate: {results['baseline_conversion_rate']:.2f}%")
    print(f"   ğŸ“Š Target Rate: {results['target_conversion_rate']:.2f}%")
    print(f"   ğŸ“ˆ Expected Improvement: +{results['expected_improvement']:.1f}%")
    print(f"   âš¡ Statistical Power: {results['statistical_power']:.0f}%")
    print(f"   ğŸ” Significance Level: {results['significance_level']:.1f}%")
    print(f"   ğŸ“ Effect Size (Cohen's h): {results['effect_size_cohens_h']:.3f}")
    print(f"   ğŸ‘¥ Sample Size per Group: {results['sample_size_per_group']:,}")
    print(f"   ğŸ“Š Total Sample Size Needed: {results['total_sample_size']:,}")

    if duration_estimates:
        print(f"\nâ±ï¸ ESTIMATED TEST DURATION:")
        for scenario, days in duration_estimates.items():
            scenario_name = scenario.replace('_', ' ').title()
            if days < 365:  # Less than a year
                print(f"   {scenario_name}: {days:.0f} days")
            else:
                print(f"   {scenario_name}: {days/365:.1f} years")

    if potential_impact:
        print(f"\nğŸ’° POTENTIAL BUSINESS IMPACT:")
        for key, value in potential_impact.items():
            print(f"   {key}: {value}")

    return results

# MAIN EXECUTION FUNCTION - RUN THIS
def run_power_analysis_fixed(ab_tester):
    """Run power analysis with fixed column handling - NO IMPORTS NEEDED"""

    print("ğŸš€ RUNNING FIXED POWER ANALYSIS ON YOUR REAL DATA")
    print("=" * 60)

    # First, inspect all your datasets
    print("\nğŸ” INSPECTING ALL YOUR DATASETS")
    print("=" * 40)

    for dataset_name in ab_tester.datasets.keys():
        inspect_dataset_columns(ab_tester, dataset_name)
        print("\n" + "-" * 50)

    # Analyze Cookie Cats retention optimization
    print("\n1ï¸âƒ£ COOKIE CATS RETENTION OPTIMIZATION (FIXED)")
    print("=" * 50)
    cookie_results = calculate_sample_size_for_your_data_fixed(
        ab_tester,
        dataset_name='cookie_cats',
        metric='retention_1',
        mde=0.15,  # 15% improvement target
        power=0.8
    )

    # Try Facebook Ads analysis with error handling
    if 'ab_facebook' in ab_tester.datasets:
        print("\n2ï¸âƒ£ FACEBOOK ADS OPTIMIZATION (FIXED)")
        print("=" * 40)
        try:
            facebook_results = calculate_sample_size_for_your_data_fixed(
                ab_tester,
                dataset_name='ab_facebook',
                metric='purchase_rate',
                mde=0.20,  # 20% improvement target
                power=0.8
            )
        except Exception as e:
            print(f"âš ï¸ Facebook analysis error: {str(e)}")
            facebook_results = None
    else:
        facebook_results = None

    # Try Digital Ads analysis
    if 'digital_ads' in ab_tester.datasets:
        print("\n3ï¸âƒ£ DIGITAL ADS OPTIMIZATION (FIXED)")
        print("=" * 35)
        try:
            digital_results = calculate_sample_size_for_your_data_fixed(
                ab_tester,
                dataset_name='digital_ads',
                metric='conversion_rate',
                mde=0.15,
                power=0.8
            )
        except Exception as e:
            print(f"âš ï¸ Digital ads analysis error: {str(e)}")
            digital_results = None
    else:
        digital_results = None

    print(f"\nğŸ‰ POWER ANALYSIS COMPLETE!")
    print(f"âœ… Cookie Cats: {'Success' if cookie_results else 'Failed'}")
    print(f"âœ… Facebook Ads: {'Success' if facebook_results else 'Failed'}")
    print(f"âœ… Digital Ads: {'Success' if digital_results else 'Failed'}")

    return {
        'cookie_cats_sample_size': cookie_results,
        'facebook_ads_sample_size': facebook_results,
        'digital_ads_sample_size': digital_results
    }

# ==========================================
# RUN THIS CODE DIRECTLY IN YOUR NOTEBOOK
# ==========================================

print("ğŸ“Š CHUNK 2 - DIRECT IMPLEMENTATION READY")
print("âœ… No imports needed - run directly in your environment")
print("\nUsage:")
print("# Assuming you have ab_tester initialized from Chunk 1:")
print("power_results = run_power_analysis_fixed(ab_tester)")

power_results = run_power_analysis_fixed(ab_tester)

"""#  Sequential Testing & Early Stopping"""

# CHUNK 3 - WORKING REAL DATA ANALYSIS
# Fixed to actually produce output with your real datasets

import pandas as pd
import numpy as np
from scipy.stats import norm
from statsmodels.stats.proportion import proportions_ztest, proportion_effectsize

def analyze_cookie_cats_real_ab_test(ab_tester):
    """
    Analyze the actual Cookie Cats A/B test (gate_30 vs gate_40)
    NO simulation - uses your real 90K users data
    """
    print("ğŸ® COOKIE CATS REAL A/B TEST ANALYSIS")
    print("=" * 50)
    print("ğŸ“Š Using your actual 90,189 users data")

    if 'cookie_cats' not in ab_tester.datasets:
        print("âŒ Cookie Cats dataset not loaded")
        return None

    data = ab_tester.datasets['cookie_cats']

    # Real A/B test groups
    control_group = data[data['version'] == 'gate_30']
    treatment_group = data[data['version'] == 'gate_40']

    print(f"\nğŸ“Š REAL DATA SUMMARY:")
    print(f"   Control (gate_30): {len(control_group):,} users")
    print(f"   Treatment (gate_40): {len(treatment_group):,} users")
    print(f"   Total users: {len(data):,}")

    # Analyze both retention metrics using REAL data
    results = {}

    # 1-day retention analysis
    print(f"\nğŸ“ˆ 1-DAY RETENTION ANALYSIS:")
    control_retention_1 = control_group['retention_1'].sum()
    control_total_1 = len(control_group)
    treatment_retention_1 = treatment_group['retention_1'].sum()
    treatment_total_1 = len(treatment_group)

    control_rate_1 = control_retention_1 / control_total_1
    treatment_rate_1 = treatment_retention_1 / treatment_total_1
    relative_change_1 = ((treatment_rate_1 - control_rate_1) / control_rate_1) * 100

    # Statistical test
    z_stat_1, p_value_1 = proportions_ztest(
        [control_retention_1, treatment_retention_1],
        [control_total_1, treatment_total_1]
    )

    print(f"   Control Rate: {control_rate_1*100:.2f}% ({control_retention_1:,}/{control_total_1:,})")
    print(f"   Treatment Rate: {treatment_rate_1*100:.2f}% ({treatment_retention_1:,}/{treatment_total_1:,})")
    print(f"   Relative Change: {relative_change_1:+.1f}%")
    print(f"   Z-statistic: {z_stat_1:.3f}")
    print(f"   P-value: {p_value_1:.4f}")
    print(f"   Statistically Significant: {'âœ… Yes' if p_value_1 < 0.05 else 'âŒ No'}")

    results['retention_1'] = {
        'control_rate': control_rate_1,
        'treatment_rate': treatment_rate_1,
        'relative_change': relative_change_1,
        'z_statistic': z_stat_1,
        'p_value': p_value_1,
        'significant': p_value_1 < 0.05,
        'control_users': control_total_1,
        'treatment_users': treatment_total_1
    }

    # 7-day retention analysis
    print(f"\nğŸ“ˆ 7-DAY RETENTION ANALYSIS:")
    control_retention_7 = control_group['retention_7'].sum()
    treatment_retention_7 = treatment_group['retention_7'].sum()

    control_rate_7 = control_retention_7 / control_total_1
    treatment_rate_7 = treatment_retention_7 / treatment_total_1
    relative_change_7 = ((treatment_rate_7 - control_rate_7) / control_rate_7) * 100

    # Statistical test
    z_stat_7, p_value_7 = proportions_ztest(
        [control_retention_7, treatment_retention_7],
        [control_total_1, treatment_total_1]
    )

    print(f"   Control Rate: {control_rate_7*100:.2f}% ({control_retention_7:,}/{control_total_1:,})")
    print(f"   Treatment Rate: {treatment_rate_7*100:.2f}% ({treatment_retention_7:,}/{treatment_total_1:,})")
    print(f"   Relative Change: {relative_change_7:+.1f}%")
    print(f"   Z-statistic: {z_stat_7:.3f}")
    print(f"   P-value: {p_value_7:.4f}")
    print(f"   Statistically Significant: {'âœ… Yes' if p_value_7 < 0.05 else 'âŒ No'}")

    results['retention_7'] = {
        'control_rate': control_rate_7,
        'treatment_rate': treatment_rate_7,
        'relative_change': relative_change_7,
        'z_statistic': z_stat_7,
        'p_value': p_value_7,
        'significant': p_value_7 < 0.05,
        'control_users': control_total_1,
        'treatment_users': treatment_total_1
    }

    # Business impact calculation
    print(f"\nğŸ’° BUSINESS IMPACT CALCULATION:")
    if relative_change_1 > 0:
        total_users = len(data)
        additional_retained_1 = total_users * control_rate_1 * (relative_change_1 / 100)
        revenue_impact_1 = additional_retained_1 * 5  # $5 per retained user

        print(f"   1-day retention improvement: +{relative_change_1:.1f}%")
        print(f"   Additional retained users: {additional_retained_1:,.0f}")
        print(f"   Estimated revenue impact: ${revenue_impact_1:,.0f}")

        results['business_impact'] = {
            'additional_users': additional_retained_1,
            'revenue_impact': revenue_impact_1,
            'retention_improvement': relative_change_1
        }

    print(f"\nğŸ¯ DATA SOURCE: 100% Real Cookie Cats A/B Test Data")
    print(f"   âœ… No synthetic data used")
    print(f"   âœ… No simulation applied")
    print(f"   âœ… Actual user behavior analysis")

    return results

def analyze_facebook_ads_real_ab_test(ab_tester):
    """
    Analyze the actual Facebook Ads A/B test
    Uses your real control vs test campaign data
    """
    print("\nğŸ’° FACEBOOK ADS REAL A/B TEST ANALYSIS")
    print("=" * 50)

    if 'ab_facebook' not in ab_tester.datasets:
        print("âŒ Facebook Ads dataset not loaded")
        return None

    control_data = ab_tester.datasets['ab_facebook']['control']
    test_data = ab_tester.datasets['ab_facebook']['test']

    print(f"ğŸ“Š REAL FACEBOOK ADS DATA:")
    print(f"   Control campaigns: {len(control_data)}")
    print(f"   Test campaigns: {len(test_data)}")
    print(f"   Columns: {list(control_data.columns)}")

    results = {}

    # Find the correct column names
    purchase_col = None
    click_col = None
    impression_col = None

    # Check for purchase columns
    for col in ['# of Purchase', 'purchases', 'Purchase']:
        if col in control_data.columns:
            purchase_col = col
            break

    # Check for click columns
    for col in ['# of Website Clicks', 'clicks', 'Clicks']:
        if col in control_data.columns:
            click_col = col
            break

    # Check for impression columns
    for col in ['# of Impressions', 'impressions', 'Impressions']:
        if col in control_data.columns:
            impression_col = col
            break

    print(f"\nğŸ“Š FOUND COLUMNS:")
    print(f"   Purchase: {purchase_col}")
    print(f"   Clicks: {click_col}")
    print(f"   Impressions: {impression_col}")

    # Analyze purchase rates if data available
    if purchase_col and impression_col:
        print(f"\nğŸ“ˆ PURCHASE RATE ANALYSIS:")

        control_purchases = control_data[purchase_col].sum()
        control_impressions = control_data[impression_col].sum()
        test_purchases = test_data[purchase_col].sum()
        test_impressions = test_data[impression_col].sum()

        control_purchase_rate = control_purchases / control_impressions if control_impressions > 0 else 0
        test_purchase_rate = test_purchases / test_impressions if test_impressions > 0 else 0
        relative_change = ((test_purchase_rate - control_purchase_rate) / control_purchase_rate * 100) if control_purchase_rate > 0 else 0

        print(f"   Control: {control_purchases:,} purchases / {control_impressions:,} impressions = {control_purchase_rate*100:.4f}%")
        print(f"   Test: {test_purchases:,} purchases / {test_impressions:,} impressions = {test_purchase_rate*100:.4f}%")
        print(f"   Relative Change: {relative_change:+.1f}%")

        # Statistical test
        if control_impressions > 0 and test_impressions > 0:
            z_stat, p_value = proportions_ztest(
                [control_purchases, test_purchases],
                [control_impressions, test_impressions]
            )
            print(f"   Z-statistic: {z_stat:.3f}")
            print(f"   P-value: {p_value:.4f}")
            print(f"   Statistically Significant: {'âœ… Yes' if p_value < 0.05 else 'âŒ No'}")

            results['purchase_rate'] = {
                'control_rate': control_purchase_rate,
                'test_rate': test_purchase_rate,
                'relative_change': relative_change,
                'z_statistic': z_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }

    # Analyze click rates if data available
    if click_col and impression_col:
        print(f"\nğŸ–±ï¸ CLICK RATE ANALYSIS:")

        control_clicks = control_data[click_col].sum()
        test_clicks = test_data[click_col].sum()

        control_click_rate = control_clicks / control_impressions if control_impressions > 0 else 0
        test_click_rate = test_clicks / test_impressions if test_impressions > 0 else 0
        relative_change_clicks = ((test_click_rate - control_click_rate) / control_click_rate * 100) if control_click_rate > 0 else 0

        print(f"   Control: {control_clicks:,} clicks / {control_impressions:,} impressions = {control_click_rate*100:.4f}%")
        print(f"   Test: {test_clicks:,} clicks / {test_impressions:,} impressions = {test_click_rate*100:.4f}%")
        print(f"   Relative Change: {relative_change_clicks:+.1f}%")

        # Statistical test
        if control_impressions > 0 and test_impressions > 0:
            z_stat_clicks, p_value_clicks = proportions_ztest(
                [control_clicks, test_clicks],
                [control_impressions, test_impressions]
            )
            print(f"   Z-statistic: {z_stat_clicks:.3f}")
            print(f"   P-value: {p_value_clicks:.4f}")
            print(f"   Statistically Significant: {'âœ… Yes' if p_value_clicks < 0.05 else 'âŒ No'}")

            results['click_rate'] = {
                'control_rate': control_click_rate,
                'test_rate': test_click_rate,
                'relative_change': relative_change_clicks,
                'z_statistic': z_stat_clicks,
                'p_value': p_value_clicks,
                'significant': p_value_clicks < 0.05
            }

    print(f"\nğŸ¯ DATA SOURCE: 100% Real Facebook Ads A/B Test Data")

    return results

def multiple_testing_correction_real_data(ab_tester):
    """
    Apply multiple testing corrections to your REAL A/B test results
    No synthetic data - uses actual p-values from your tests
    """
    print("\nğŸ”¬ MULTIPLE TESTING CORRECTION - REAL DATA")
    print("=" * 50)

    # Collect real p-values from your actual A/B tests
    real_p_values = {}

    # Get Cookie Cats results
    cookie_results = analyze_cookie_cats_real_ab_test(ab_tester)
    if cookie_results:
        real_p_values['cookie_cats_retention_1'] = cookie_results['retention_1']['p_value']
        real_p_values['cookie_cats_retention_7'] = cookie_results['retention_7']['p_value']

    # Get Facebook Ads results
    facebook_results = analyze_facebook_ads_real_ab_test(ab_tester)
    if facebook_results:
        if 'purchase_rate' in facebook_results:
            real_p_values['facebook_purchase_rate'] = facebook_results['purchase_rate']['p_value']
        if 'click_rate' in facebook_results:
            real_p_values['facebook_click_rate'] = facebook_results['click_rate']['p_value']

    if not real_p_values:
        print("âŒ No real A/B test results available for correction")
        return None

    print(f"\nğŸ“Š REAL P-VALUES FROM YOUR TESTS:")
    for test_name, p_val in real_p_values.items():
        significant = "âœ…" if p_val < 0.05 else "âŒ"
        print(f"   {test_name}: p = {p_val:.4f} {significant}")

    # Apply Bonferroni correction
    n_tests = len(real_p_values)
    bonferroni_alpha = 0.05 / n_tests

    print(f"\nğŸ”¬ BONFERRONI CORRECTION:")
    print(f"   Number of tests: {n_tests}")
    print(f"   Original Î±: 0.05")
    print(f"   Corrected Î±: {bonferroni_alpha:.4f}")

    bonferroni_significant = []
    for test_name, p_val in real_p_values.items():
        significant = p_val < bonferroni_alpha
        bonferroni_significant.append(significant)
        status = "âœ… Significant" if significant else "âŒ Not Significant"
        print(f"   {test_name}: {status}")

    # Apply Benjamini-Hochberg correction
    from statsmodels.stats.multitest import multipletests

    p_values_list = list(real_p_values.values())
    test_names_list = list(real_p_values.keys())

    bh_significant, bh_p_corrected, _, _ = multipletests(
        p_values_list,
        alpha=0.05,
        method='fdr_bh'
    )

    print(f"\nğŸ”¬ BENJAMINI-HOCHBERG CORRECTION (FDR):")
    for i, test_name in enumerate(test_names_list):
        status = "âœ… Significant" if bh_significant[i] else "âŒ Not Significant"
        print(f"   {test_name}: {status} (corrected p = {bh_p_corrected[i]:.4f})")

    # Summary
    original_significant = sum(1 for p in p_values_list if p < 0.05)
    bonferroni_count = sum(bonferroni_significant)
    bh_count = sum(bh_significant)

    print(f"\nğŸ“Š CORRECTION SUMMARY:")
    print(f"   Original significant tests: {original_significant}/{n_tests}")
    print(f"   Bonferroni significant: {bonferroni_count}/{n_tests}")
    print(f"   Benjamini-Hochberg significant: {bh_count}/{n_tests}")

    print(f"\nğŸ¯ DATA SOURCE: 100% Real A/B Test P-values")
    print(f"   âœ… No synthetic p-values")
    print(f"   âœ… Actual statistical test results")

    return {
        'original_p_values': real_p_values,
        'bonferroni_alpha': bonferroni_alpha,
        'bonferroni_significant': bonferroni_significant,
        'bh_significant': bh_significant.tolist(),
        'bh_p_corrected': bh_p_corrected.tolist(),
        'summary': {
            'original_significant': original_significant,
            'bonferroni_significant': bonferroni_count,
            'bh_significant': bh_count,
            'total_tests': n_tests
        }
    }

def run_complete_real_data_analysis(ab_tester):
    """
    Run complete A/B testing analysis using ONLY your real datasets
    NO synthetic data, NO simulation - 100% real analysis
    """
    print("ğŸš€ COMPLETE REAL DATA A/B TESTING ANALYSIS")
    print("=" * 60)
    print("ğŸ¯ Using ONLY your actual datasets")
    print("âŒ No synthetic data")
    print("âŒ No simulation")
    print("âœ… 100% real user behavior analysis")
    print("=" * 60)

    results = {}

    # 1. Cookie Cats Real Analysis
    print("\n1ï¸âƒ£ COOKIE CATS ANALYSIS")
    print("=" * 30)
    cookie_results = analyze_cookie_cats_real_ab_test(ab_tester)
    if cookie_results:
        results['cookie_cats'] = cookie_results

    # 2. Facebook Ads Real Analysis
    print("\n2ï¸âƒ£ FACEBOOK ADS ANALYSIS")
    print("=" * 30)
    facebook_results = analyze_facebook_ads_real_ab_test(ab_tester)
    if facebook_results:
        results['facebook_ads'] = facebook_results

    # 3. Multiple Testing Correction
    print("\n3ï¸âƒ£ MULTIPLE TESTING CORRECTION")
    print("=" * 35)
    correction_results = multiple_testing_correction_real_data(ab_tester)
    if correction_results:
        results['multiple_testing'] = correction_results

    # 4. Summary
    print("\nğŸ‰ COMPLETE ANALYSIS SUMMARY")
    print("=" * 35)

    total_tests = 0
    significant_tests = 0

    if 'cookie_cats' in results:
        if results['cookie_cats']['retention_1']['significant']:
            print("âœ… Cookie Cats 1-day retention: SIGNIFICANT improvement")
            significant_tests += 1
        else:
            print("âŒ Cookie Cats 1-day retention: Not significant")
        total_tests += 1

        if results['cookie_cats']['retention_7']['significant']:
            print("âœ… Cookie Cats 7-day retention: SIGNIFICANT improvement")
            significant_tests += 1
        else:
            print("âŒ Cookie Cats 7-day retention: Not significant")
        total_tests += 1

    if 'facebook_ads' in results:
        for metric, result in results['facebook_ads'].items():
            if result['significant']:
                print(f"âœ… Facebook Ads {metric}: SIGNIFICANT improvement")
                significant_tests += 1
            else:
                print(f"âŒ Facebook Ads {metric}: Not significant")
            total_tests += 1

    print(f"\nğŸ“Š FINAL RESULTS:")
    print(f"   Total A/B tests analyzed: {total_tests}")
    print(f"   Statistically significant: {significant_tests}")
    print(f"   Success rate: {(significant_tests/total_tests)*100:.1f}%" if total_tests > 0 else "   No tests completed")

    print(f"\nğŸ¯ DATA AUTHENTICITY:")
    print(f"   âœ… Real datasets used: {len(ab_tester.datasets)}")
    print(f"   âœ… Real users analyzed: 90,000+ (Cookie Cats)")
    print(f"   âœ… Real campaign data: Facebook Ads A/B test")
    print(f"   âŒ Synthetic data: 0%")
    print(f"   âŒ Simulated data: 0%")

    return results

# ==========================================
# MAIN EXECUTION - RUN THIS
# ==========================================

print("ğŸ® CHUNK 3 - REAL DATA A/B TESTING ANALYSIS")
print("=" * 50)
print("âœ… Fixed to produce actual output")
print("âœ… Uses only your real datasets")
print("âœ… No synthetic or simulated data")
print("\nUsage:")
print("# Run this to analyze your real A/B tests:")
print("real_results = run_complete_real_data_analysis(ab_tester)")

real_results = run_complete_real_data_analysis(ab_tester)





"""## Bayesian A/B Testing Analysis"""

# CHUNK 4: BAYESIAN A/B TESTING ANALYSIS
# Probabilistic approach to A/B testing with your real datasets

def add_bayesian_methods(ab_tester_class):
    """Add Bayesian analysis methods to the AdvancedABTesting class"""

    def bayesian_ab_analysis(self, dataset_name, metric,
                           prior_alpha=1, prior_beta=1,
                           n_simulations=100000, credible_interval=0.95):
        """
        Comprehensive Bayesian A/B testing analysis using Beta-Binomial conjugate priors

        Parameters:
        - dataset_name: Your dataset ('cookie_cats', 'ab_facebook', 'digital_ads')
        - metric: The metric to analyze
        - prior_alpha, prior_beta: Beta prior parameters (uninformative by default)
        - n_simulations: Monte Carlo simulations for probability calculations
        - credible_interval: Credible interval width (e.g., 0.95 for 95% CI)
        """

        print("ğŸ”® BAYESIAN A/B TEST ANALYSIS")
        print("=" * 45)
        print(f"Dataset: {dataset_name} | Metric: {metric}")

        # Extract control and treatment data
        control_data, treatment_data = self._extract_ab_data(dataset_name, metric)

        if control_data is None or treatment_data is None:
            print("âŒ Could not extract A/B test data")
            return None

        control_conversions = control_data['conversions']
        control_total = control_data['total']
        treatment_conversions = treatment_data['conversions']
        treatment_total = treatment_data['total']

        print(f"ğŸ“Š DATA SUMMARY:")
        print(f"   Control: {control_conversions}/{control_total} conversions ({control_conversions/control_total*100:.2f}%)")
        print(f"   Treatment: {treatment_conversions}/{treatment_total} conversions ({treatment_conversions/treatment_total*100:.2f}%)")

        # Calculate posterior parameters using Beta-Binomial conjugacy
        control_alpha_post = prior_alpha + control_conversions
        control_beta_post = prior_beta + control_total - control_conversions

        treatment_alpha_post = prior_alpha + treatment_conversions
        treatment_beta_post = prior_beta + treatment_total - treatment_conversions

        # Posterior means (expected conversion rates)
        control_mean = control_alpha_post / (control_alpha_post + control_beta_post)
        treatment_mean = treatment_alpha_post / (treatment_alpha_post + treatment_beta_post)

        print(f"\nğŸ¯ POSTERIOR ESTIMATES:")
        print(f"   Control Rate: {control_mean*100:.2f}%")
        print(f"   Treatment Rate: {treatment_mean*100:.2f}%")
        print(f"   Relative Improvement: {((treatment_mean - control_mean)/control_mean)*100:+.1f}%")

        # Monte Carlo simulation for probability calculations
        np.random.seed(42)  # For reproducibility

        # Sample from posterior distributions
        control_samples = np.random.beta(control_alpha_post, control_beta_post, n_simulations)
        treatment_samples = np.random.beta(treatment_alpha_post, treatment_beta_post, n_simulations)

        # Calculate key probabilities
        prob_treatment_better = np.mean(treatment_samples > control_samples)
        prob_control_better = 1 - prob_treatment_better

        # Expected loss calculations (decision theory)
        loss_if_choose_treatment = np.mean(np.maximum(control_samples - treatment_samples, 0))
        loss_if_choose_control = np.mean(np.maximum(treatment_samples - control_samples, 0))

        # Relative improvement distribution
        relative_improvement = (treatment_samples - control_samples) / control_samples

        # Credible intervals
        ci_lower = np.percentile(relative_improvement, (1-credible_interval)/2 * 100)
        ci_upper = np.percentile(relative_improvement, (1-(1-credible_interval)/2) * 100)

        # Risk assessments
        prob_negative_effect = np.mean(relative_improvement < 0) * 100
        prob_small_effect = np.mean(np.abs(relative_improvement) < 0.01) * 100  # <1% change
        prob_medium_effect = np.mean((relative_improvement >= 0.01) & (relative_improvement < 0.1)) * 100  # 1-10%
        prob_large_effect = np.mean(relative_improvement >= 0.1) * 100  # >10% improvement

        # Value at Risk (VaR) - worst case scenarios
        var_5pct = np.percentile(relative_improvement, 5) * 100  # 5% VaR
        var_1pct = np.percentile(relative_improvement, 1) * 100  # 1% VaR

        # Probability of practical significance (minimum worthwhile effect)
        min_worthwhile_effect = 0.02  # 2% minimum improvement
        prob_practically_significant = np.mean(relative_improvement > min_worthwhile_effect) * 100

        # Decision recommendation based on thresholds
        recommendation = self._get_bayesian_recommendation(
            prob_treatment_better, loss_if_choose_treatment, loss_if_choose_control,
            prob_practically_significant, prob_negative_effect
        )

        # Business impact estimation
        business_impact = self._calculate_bayesian_business_impact(
            dataset_name, metric, treatment_mean - control_mean, relative_improvement
        )

        results = {
            'dataset': dataset_name,
            'metric': metric,
            'data_summary': {
                'control_conversions': control_conversions,
                'control_total': control_total,
                'treatment_conversions': treatment_conversions,
                'treatment_total': treatment_total
            },
            'posterior_estimates': {
                'control_mean': control_mean * 100,
                'treatment_mean': treatment_mean * 100,
                'absolute_difference': (treatment_mean - control_mean) * 100,
                'relative_improvement': ((treatment_mean - control_mean)/control_mean) * 100
            },
            'probabilities': {
                'treatment_better': prob_treatment_better * 100,
                'control_better': prob_control_better * 100,
                'practically_significant': prob_practically_significant
            },
            'expected_loss': {
                'if_choose_treatment': loss_if_choose_treatment * 100,
                'if_choose_control': loss_if_choose_control * 100
            },
            'credible_interval': {
                'lower': ci_lower * 100,
                'upper': ci_upper * 100,
                'width': credible_interval * 100
            },
            'risk_assessment': {
                'prob_negative_effect': prob_negative_effect,
                'prob_small_effect': prob_small_effect,
                'prob_medium_effect': prob_medium_effect,
                'prob_large_effect': prob_large_effect,
                'var_5pct': var_5pct,
                'var_1pct': var_1pct
            },
            'recommendation': recommendation,
            'business_impact': business_impact,
            'samples': {
                'control': control_samples,
                'treatment': treatment_samples,
                'relative_improvement': relative_improvement
            }
        }

        # Display comprehensive results
        print(f"\nğŸ“ˆ PROBABILITY ANALYSIS:")
        print(f"   Probability Treatment is Better: {results['probabilities']['treatment_better']:.1f}%")
        print(f"   Probability Control is Better: {results['probabilities']['control_better']:.1f}%")
        print(f"   Probability of Practical Significance: {results['probabilities']['practically_significant']:.1f}%")

        print(f"\nğŸ’° EXPECTED LOSS (Decision Theory):")
        print(f"   Loss if Choose Treatment: {results['expected_loss']['if_choose_treatment']:.3f}%")
        print(f"   Loss if Choose Control: {results['expected_loss']['if_choose_control']:.3f}%")

        print(f"\nğŸ“Š {credible_interval*100:.0f}% CREDIBLE INTERVAL (Relative Improvement):")
        print(f"   Lower Bound: {results['credible_interval']['lower']:+.1f}%")
        print(f"   Upper Bound: {results['credible_interval']['upper']:+.1f}%")

        print(f"\nâš ï¸ RISK ASSESSMENT:")
        print(f"   Risk of Negative Effect: {results['risk_assessment']['prob_negative_effect']:.1f}%")
        print(f"   Probability of Small Effect (<1%): {results['risk_assessment']['prob_small_effect']:.1f}%")
        print(f"   Probability of Medium Effect (1-10%): {results['risk_assessment']['prob_medium_effect']:.1f}%")
        print(f"   Probability of Large Effect (>10%): {results['risk_assessment']['prob_large_effect']:.1f}%")
        print(f"   Value at Risk (5%): {results['risk_assessment']['var_5pct']:+.1f}%")
        print(f"   Value at Risk (1%): {results['risk_assessment']['var_1pct']:+.1f}%")

        print(f"\nğŸ¯ BAYESIAN RECOMMENDATION: {results['recommendation']}")

        if business_impact:
            print(f"\nğŸ’¼ BUSINESS IMPACT ESTIMATION:")
            for key, value in business_impact.items():
                print(f"   {key}: {value}")

        return results

    def _extract_ab_data(self, dataset_name, metric):
        """Extract control and treatment data for Bayesian analysis"""

        if dataset_name == 'cookie_cats':
            data = self.datasets['cookie_cats']

            control_data = data[data['version'] == 'gate_30']
            treatment_data = data[data['version'] == 'gate_40']

            if metric == 'retention_1':
                control_conversions = control_data['retention_1'].sum()
                treatment_conversions = treatment_data['retention_1'].sum()
            elif metric == 'retention_7':
                control_conversions = control_data['retention_7'].sum()
                treatment_conversions = treatment_data['retention_7'].sum()
            else:
                return None, None

            return {
                'conversions': control_conversions,
                'total': len(control_data)
            }, {
                'conversions': treatment_conversions,
                'total': len(treatment_data)
            }

        elif dataset_name == 'ab_facebook':
            control_df = self.datasets['ab_facebook']['control']
            treatment_df = self.datasets['ab_facebook']['test']

            if metric == 'purchase_rate':
                control_conversions = control_df['# of Purchase'].sum()
                control_total = control_df['# of Impressions'].sum()
                treatment_conversions = treatment_df['# of Purchase'].sum()
                treatment_total = treatment_df['# of Impressions'].sum()
            elif metric == 'click_rate':
                control_conversions = control_df['# of Website Clicks'].sum()
                control_total = control_df['# of Impressions'].sum()
                treatment_conversions = treatment_df['# of Website Clicks'].sum()
                treatment_total = treatment_df['# of Impressions'].sum()
            else:
                return None, None

            return {
                'conversions': control_conversions,
                'total': control_total
            }, {
                'conversions': treatment_conversions,
                'total': treatment_total
            }

        return None, None

    def _get_bayesian_recommendation(self, prob_treatment_better, loss_treatment,
                                   loss_control, prob_practical, prob_negative):
        """Generate Bayesian recommendation based on probabilities and losses"""

        # Decision thresholds
        high_confidence_threshold = 95  # 95% probability
        medium_confidence_threshold = 80  # 80% probability
        practical_significance_threshold = 70  # 70% probability
        max_acceptable_risk = 20  # 20% risk of negative effect

        if prob_treatment_better > high_confidence_threshold and prob_negative < max_acceptable_risk:
            if prob_practical > practical_significance_threshold:
                return "ğŸŸ¢ STRONG RECOMMENDATION: Implement Treatment"
            else:
                return "ğŸŸ¡ WEAK POSITIVE: Implement Treatment (Low Practical Impact)"
        elif prob_treatment_better > medium_confidence_threshold and prob_negative < max_acceptable_risk:
            return "ğŸŸ  MODERATE RECOMMENDATION: Consider Implementing Treatment"
        elif prob_treatment_better < (100 - high_confidence_threshold):
            return "ğŸ”´ RECOMMENDATION: Keep Control (Treatment Likely Harmful)"
        else:
            return "âšª INCONCLUSIVE: Collect More Data or Run Longer"

    def _calculate_bayesian_business_impact(self, dataset_name, metric, absolute_diff, rel_improvement_samples):
        """Calculate business impact using Bayesian posterior samples"""

        impact = {}

        if dataset_name == 'cookie_cats':
            total_users = len(self.datasets['cookie_cats'])

            # Calculate distribution of additional users
            additional_users_samples = total_users * absolute_diff

            # Revenue per user estimates
            revenue_per_user = 5  # $5 lifetime value

            # Calculate revenue impact distribution
            revenue_impact_samples = additional_users_samples * revenue_per_user

            impact['Expected Additional Users'] = f"{np.mean(additional_users_samples):,.0f}"
            impact['95% CI Additional Users'] = f"[{np.percentile(additional_users_samples, 2.5):,.0f}, {np.percentile(additional_users_samples, 97.5):,.0f}]"
            impact['Expected Revenue Impact'] = f"${np.mean(revenue_impact_samples):,.0f}"
            impact['95% CI Revenue Impact'] = f"[${np.percentile(revenue_impact_samples, 2.5):,.0f}, ${np.percentile(revenue_impact_samples, 97.5):,.0f}]"
            impact['Probability of Positive ROI'] = f"{np.mean(revenue_impact_samples > 0)*100:.1f}%"

        elif dataset_name == 'ab_facebook':
            control_data = self.datasets['ab_facebook']['control']
            baseline_purchases = control_data['# of Purchase'].sum()

            # Additional purchases distribution
            additional_purchases_samples = baseline_purchases * rel_improvement_samples

            # Revenue per purchase
            revenue_per_purchase = 50  # $50 average order value

            revenue_impact_samples = additional_purchases_samples * revenue_per_purchase

            impact['Expected Additional Purchases'] = f"{np.mean(additional_purchases_samples):.1f}"
            impact['95% CI Additional Purchases'] = f"[{np.percentile(additional_purchases_samples, 2.5):.1f}, {np.percentile(additional_purchases_samples, 97.5):.1f}]"
            impact['Expected Revenue Impact'] = f"${np.mean(revenue_impact_samples):,.0f}"
            impact['Probability of Revenue Increase'] = f"{np.mean(revenue_impact_samples > 0)*100:.1f}%"

        return impact

    def bayesian_multivariate_analysis(self, dataset_name, metrics_list):
        """Analyze multiple metrics simultaneously using Bayesian approach"""

        print("ğŸ”® BAYESIAN MULTIVARIATE ANALYSIS")
        print("=" * 45)
        print(f"Dataset: {dataset_name}")
        print(f"Metrics: {', '.join(metrics_list)}")

        results = {}

        for metric in metrics_list:
            print(f"\n--- Analyzing {metric} ---")
            result = self.bayesian_ab_analysis(dataset_name, metric, n_simulations=50000)
            if result:
                results[metric] = result

        # Cross-metric analysis
        if len(results) > 1:
            print(f"\nğŸ”„ CROSS-METRIC ANALYSIS:")

            metric_names = list(results.keys())
            for i, metric1 in enumerate(metric_names):
                for metric2 in metric_names[i+1:]:

                    # Calculate correlation between metrics
                    samples1 = results[metric1]['samples']['relative_improvement']
                    samples2 = results[metric2]['samples']['relative_improvement']

                    correlation = np.corrcoef(samples1, samples2)[0,1]

                    # Joint probability of both metrics improving
                    joint_improvement = np.mean((samples1 > 0) & (samples2 > 0)) * 100

                    print(f"   {metric1} vs {metric2}:")
                    print(f"     Correlation: {correlation:.3f}")
                    print(f"     Joint Improvement Probability: {joint_improvement:.1f}%")

        return results

    def bayesian_value_of_information(self, dataset_name, metric, additional_samples_range):
        """Calculate the value of collecting additional data"""

        print("ğŸ’° BAYESIAN VALUE OF INFORMATION ANALYSIS")
        print("=" * 50)

        # Current analysis
        current_results = self.bayesian_ab_analysis(dataset_name, metric, n_simulations=10000)
        if not current_results:
            return None

        current_uncertainty = current_results['credible_interval']['upper'] - current_results['credible_interval']['lower']
        current_prob_better = current_results['probabilities']['treatment_better']

        print(f"\nğŸ“Š CURRENT STATE:")
        print(f"   Probability Treatment Better: {current_prob_better:.1f}%")
        print(f"   Credible Interval Width: {current_uncertainty:.1f}%")

        voi_results = []

        for additional_samples in additional_samples_range:
            # Simulate additional data collection
            # This is a simplified VOI calculation

            # Estimate how much uncertainty would be reduced
            current_total = (current_results['data_summary']['control_total'] +
                           current_results['data_summary']['treatment_total'])
            new_total = current_total + additional_samples

            # Approximate uncertainty reduction
            uncertainty_reduction_factor = np.sqrt(current_total / new_total)
            new_uncertainty = current_uncertainty * uncertainty_reduction_factor

            # Estimate probability of changing decision
            prob_decision_change = self._estimate_decision_change_probability(
                current_prob_better, current_uncertainty, new_uncertainty
            )

            # Value calculation (simplified)
            cost_per_sample = 1  # $1 per additional sample
            total_cost = additional_samples * cost_per_sample

            # Expected value of perfect information
            evpi = prob_decision_change * 1000  # $1000 value if decision changes

            net_value = evpi - total_cost

            voi_results.append({
                'additional_samples': additional_samples,
                'new_total_samples': new_total,
                'uncertainty_reduction': current_uncertainty - new_uncertainty,
                'new_uncertainty': new_uncertainty,
                'prob_decision_change': prob_decision_change * 100,
                'expected_value': evpi,
                'cost': total_cost,
                'net_value': net_value
            })

        # Find optimal additional sample size
        optimal_idx = np.argmax([r['net_value'] for r in voi_results])
        optimal_result = voi_results[optimal_idx]

        print(f"\nğŸ“ˆ VALUE OF INFORMATION RESULTS:")
        print(f"   Optimal Additional Samples: {optimal_result['additional_samples']:,}")
        print(f"   Expected Net Value: ${optimal_result['net_value']:.0f}")
        print(f"   Uncertainty Reduction: {optimal_result['uncertainty_reduction']:.1f}%")
        print(f"   Probability of Decision Change: {optimal_result['prob_decision_change']:.1f}%")

        return voi_results

    def _estimate_decision_change_probability(self, current_prob, current_uncertainty, new_uncertainty):
        """Estimate probability that additional data would change the decision"""

        # Simplified estimation based on how close we are to decision boundaries
        # and how much uncertainty would be reduced

        distance_from_boundary = abs(current_prob - 50)  # Distance from 50% (indecision)
        uncertainty_improvement = (current_uncertainty - new_uncertainty) / current_uncertainty

        # Higher chance of decision change if we're close to boundary and uncertainty reduces significantly
        prob_change = (1 - distance_from_boundary/50) * uncertainty_improvement * 0.5

        return np.clip(prob_change, 0, 0.5)  # Cap at 50% max probability

    # Add methods to the class
    ab_tester_class.bayesian_ab_analysis = bayesian_ab_analysis
    ab_tester_class._extract_ab_data = _extract_ab_data
    ab_tester_class._get_bayesian_recommendation = _get_bayesian_recommendation
    ab_tester_class._calculate_bayesian_business_impact = _calculate_bayesian_business_impact
    ab_tester_class.bayesian_multivariate_analysis = bayesian_multivariate_analysis
    ab_tester_class.bayesian_value_of_information = bayesian_value_of_information
    ab_tester_class._estimate_decision_change_probability = _estimate_decision_change_probability

# Example usage functions
def run_bayesian_analysis_suite(ab_tester):
    """Run comprehensive Bayesian analysis on your datasets"""

    print("ğŸš€ COMPREHENSIVE BAYESIAN ANALYSIS SUITE")
    print("=" * 60)

    # Add Bayesian methods
    add_bayesian_methods(ab_tester.__class__)

    # Analysis 1: Cookie Cats retention analysis
    print("\n1ï¸âƒ£ COOKIE CATS BAYESIAN ANALYSIS")
    print("=" * 40)

    cookie_retention_1 = ab_tester.bayesian_ab_analysis(
        dataset_name='cookie_cats',
        metric='retention_1',
        credible_interval=0.95
    )

    print("\n" + "="*60)

    # Analysis 2: Cookie Cats 7-day retention
    print("\n2ï¸âƒ£ COOKIE CATS 7-DAY RETENTION")
    print("=" * 35)

    cookie_retention_7 = ab_tester.bayesian_ab_analysis(
        dataset_name='cookie_cats',
        metric='retention_7',
        credible_interval=0.95
    )

    print("\n" + "="*60)

    # Analysis 3: Multivariate analysis
    print("\n3ï¸âƒ£ MULTIVARIATE BAYESIAN ANALYSIS")
    print("=" * 40)

    multivariate_results = ab_tester.bayesian_multivariate_analysis(
        dataset_name='cookie_cats',
        metrics_list=['retention_1', 'retention_7']
    )

    print("\n" + "="*60)

    # Analysis 4: Value of Information
    print("\n4ï¸âƒ£ VALUE OF INFORMATION ANALYSIS")
    print("=" * 40)

    voi_results = ab_tester.bayesian_value_of_information(
        dataset_name='cookie_cats',
        metric='retention_1',
        additional_samples_range=[1000, 5000, 10000, 25000, 50000]
    )

    return {
        'cookie_retention_1': cookie_retention_1,
        'cookie_retention_7': cookie_retention_7,
        'multivariate_analysis': multivariate_results,
        'value_of_information': voi_results
    }

def create_bayesian_visualization(bayesian_results):
    """Create visualizations for Bayesian analysis results"""

    if not bayesian_results or 'samples' not in bayesian_results:
        print("âŒ No Bayesian results with samples available")
        return None

    print("ğŸ“Š CREATING BAYESIAN VISUALIZATION DASHBOARD")
    print("=" * 50)

    # Extract samples
    control_samples = bayesian_results['samples']['control']
    treatment_samples = bayesian_results['samples']['treatment']
    rel_improvement_samples = bayesian_results['samples']['relative_improvement']

    # Create subplots
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            'Posterior Distributions',
            'Relative Improvement Distribution',
            'Probability of Being Better',
            'Risk Assessment'
        ),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )

    # Posterior distributions
    fig.add_trace(
        go.Histogram(
            x=control_samples * 100,
            name='Control',
            opacity=0.7,
            nbinsx=50,
            histnorm='probability density'
        ),
        row=1, col=1
    )

    fig.add_trace(
        go.Histogram(
            x=treatment_samples * 100,
            name='Treatment',
            opacity=0.7,
            nbinsx=50,
            histnorm='probability density'
        ),
        row=1, col=1
    )

    # Relative improvement distribution
    fig.add_trace(
        go.Histogram(
            x=rel_improvement_samples * 100,
            name='Relative Improvement',
            opacity=0.8,
            nbinsx=50,
            histnorm='probability density'
        ),
        row=1, col=2
    )

    # Add vertical line at zero
    fig.add_vline(x=0, line_dash="dash", line_color="red",
                 annotation_text="No Effect", row=1, col=2)

    # Probability visualization
    prob_better = np.mean(treatment_samples > control_samples) * 100
    prob_worse = 100 - prob_better

    fig.add_trace(
        go.Bar(
            x=['Treatment Better', 'Control Better'],
            y=[prob_better, prob_worse],
            name='Probabilities',
            marker_color=['green', 'red']
        ),
        row=2, col=1
    )

    # Risk assessment
    risk_categories = ['Negative Effect', 'Small Effect', 'Medium Effect', 'Large Effect']
    risk_probs = [
        bayesian_results['risk_assessment']['prob_negative_effect'],
        bayesian_results['risk_assessment']['prob_small_effect'],
        bayesian_results['risk_assessment']['prob_medium_effect'],
        bayesian_results['risk_assessment']['prob_large_effect']
    ]

    fig.add_trace(
        go.Bar(
            x=risk_categories,
            y=risk_probs,
            name='Risk Assessment',
            marker_color=['red', 'orange', 'yellow', 'green']
        ),
        row=2, col=2
    )

    fig.update_layout(
        title=f"Bayesian A/B Testing Analysis - {bayesian_results['dataset']} ({bayesian_results['metric']})",
        showlegend=True,
        height=800
    )

    return fig

if __name__ == "__main__":
    print("ğŸ”® ENHANCED A/B TESTING FRAMEWORK - CHUNK 4")
    print("Bayesian A/B Testing Analysis")
    print("\nUsage:")
    print("# After running Chunks 1-3:")
    print("bayesian_results = run_bayesian_analysis_suite(ab_tester)")
    print("bayesian_viz = create_bayesian_visualization(bayesian_results['cookie_retention_1'])")

bayesian_results = run_bayesian_analysis_suite(ab_tester)

bayesian_viz = create_bayesian_visualization(bayesian_results['cookie_retention_1'])

"""## Multiple Testing Corrections & Meta-Analysis"""

# CHUNK 5 FIXED: MULTIPLE TESTING CORRECTIONS - REAL DATA ONLY
# Uses ONLY your real datasets, no simulation

import pandas as pd
import numpy as np
from scipy.stats import norm, chi2
from statsmodels.stats.multitest import multipletests
from statsmodels.stats.proportion import proportions_ztest
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def analyze_multiple_real_ab_tests(ab_tester):
    """
    Analyze multiple real A/B tests from your datasets
    NO simulation - uses actual results from your data
    """
    print("ğŸ”¬ MULTIPLE REAL A/B TESTS ANALYSIS")
    print("=" * 50)
    print("ğŸ“Š Using ONLY your real datasets")
    print("âŒ No synthetic data")
    print("âŒ No simulation")

    real_test_results = {}

    # 1. Cookie Cats - Multiple metrics (REAL DATA)
    if 'cookie_cats' in ab_tester.datasets:
        print("\nğŸ“± ANALYZING COOKIE CATS REAL A/B TESTS:")

        data = ab_tester.datasets['cookie_cats']
        control_group = data[data['version'] == 'gate_30']
        treatment_group = data[data['version'] == 'gate_40']

        # Test 1: 1-day retention
        control_ret1 = control_group['retention_1'].sum()
        control_total1 = len(control_group)
        treatment_ret1 = treatment_group['retention_1'].sum()
        treatment_total1 = len(treatment_group)

        z_stat1, p_value1 = proportions_ztest(
            [control_ret1, treatment_ret1],
            [control_total1, treatment_total1]
        )

        real_test_results['cookie_cats_retention_1'] = {
            'p_value': p_value1,
            'z_statistic': z_stat1,
            'control_rate': (control_ret1 / control_total1) * 100,
            'treatment_rate': (treatment_ret1 / treatment_total1) * 100,
            'significant': p_value1 < 0.05,
            'test_name': 'Cookie Cats 1-Day Retention',
            'data_source': 'Real Cookie Cats A/B Test'
        }

        print(f"   1-Day Retention: p = {p_value1:.4f} {'âœ…' if p_value1 < 0.05 else 'âŒ'}")

        # Test 2: 7-day retention
        control_ret7 = control_group['retention_7'].sum()
        treatment_ret7 = treatment_group['retention_7'].sum()

        z_stat7, p_value7 = proportions_ztest(
            [control_ret7, treatment_ret7],
            [control_total1, treatment_total1]
        )

        real_test_results['cookie_cats_retention_7'] = {
            'p_value': p_value7,
            'z_statistic': z_stat7,
            'control_rate': (control_ret7 / control_total1) * 100,
            'treatment_rate': (treatment_ret7 / treatment_total1) * 100,
            'significant': p_value7 < 0.05,
            'test_name': 'Cookie Cats 7-Day Retention',
            'data_source': 'Real Cookie Cats A/B Test'
        }

        print(f"   7-Day Retention: p = {p_value7:.4f} {'âœ…' if p_value7 < 0.05 else 'âŒ'}")

    # 2. Facebook Ads - Multiple metrics (REAL DATA)
    if 'ab_facebook' in ab_tester.datasets:
        print("\nğŸ’° ANALYZING FACEBOOK ADS REAL A/B TESTS:")

        control_data = ab_tester.datasets['ab_facebook']['control']
        test_data = ab_tester.datasets['ab_facebook']['test']

        # Find actual column names
        purchase_col = None
        click_col = None
        impression_col = None

        for col in ['# of Purchase', 'purchases', 'Purchase']:
            if col in control_data.columns:
                purchase_col = col
                break

        for col in ['# of Website Clicks', 'clicks', 'Clicks']:
            if col in control_data.columns:
                click_col = col
                break

        for col in ['# of Impressions', 'impressions', 'Impressions']:
            if col in control_data.columns:
                impression_col = col
                break

        # Test 3: Purchase rate (if available)
        if purchase_col and impression_col:
            control_purchases = control_data[purchase_col].sum()
            control_impressions = control_data[impression_col].sum()
            test_purchases = test_data[purchase_col].sum()
            test_impressions = test_data[impression_col].sum()

            z_stat_purch, p_value_purch = proportions_ztest(
                [control_purchases, test_purchases],
                [control_impressions, test_impressions]
            )

            real_test_results['facebook_purchase_rate'] = {
                'p_value': p_value_purch,
                'z_statistic': z_stat_purch,
                'control_rate': (control_purchases / control_impressions) * 100,
                'treatment_rate': (test_purchases / test_impressions) * 100,
                'significant': p_value_purch < 0.05,
                'test_name': 'Facebook Ads Purchase Rate',
                'data_source': 'Real Facebook Ads Campaign'
            }

            print(f"   Purchase Rate: p = {p_value_purch:.4f} {'âœ…' if p_value_purch < 0.05 else 'âŒ'}")

        # Test 4: Click rate (if available)
        if click_col and impression_col:
            control_clicks = control_data[click_col].sum()
            test_clicks = test_data[click_col].sum()

            z_stat_click, p_value_click = proportions_ztest(
                [control_clicks, test_clicks],
                [control_impressions, test_impressions]
            )

            real_test_results['facebook_click_rate'] = {
                'p_value': p_value_click,
                'z_statistic': z_stat_click,
                'control_rate': (control_clicks / control_impressions) * 100,
                'treatment_rate': (test_clicks / test_impressions) * 100,
                'significant': p_value_click < 0.05,
                'test_name': 'Facebook Ads Click Rate',
                'data_source': 'Real Facebook Ads Campaign'
            }

            print(f"   Click Rate: p = {p_value_click:.4f} {'âœ…' if p_value_click < 0.05 else 'âŒ'}")

    # 3. Digital Ads - Conversion rate (REAL DATA)
    if 'digital_ads' in ab_tester.datasets:
        print("\nğŸ“Š ANALYZING DIGITAL ADS CONVERSION PERFORMANCE:")

        data = ab_tester.datasets['digital_ads']

        # Create A/B test by splitting data (e.g., by campaign or demographic)
        if 'age' in data.columns:
            # Compare different age groups as A/B test
            young_group = data[data['age'] == '30-34']
            older_group = data[data['age'] == '35-39']

            if len(young_group) > 0 and len(older_group) > 0:
                young_conversions = young_group['Total_Conversion'].sum()
                young_impressions = young_group['Impressions'].sum()
                older_conversions = older_group['Total_Conversion'].sum()
                older_impressions = older_group['Impressions'].sum()

                if young_impressions > 0 and older_impressions > 0:
                    z_stat_age, p_value_age = proportions_ztest(
                        [young_conversions, older_conversions],
                        [young_impressions, older_impressions]
                    )

                    real_test_results['digital_ads_age_comparison'] = {
                        'p_value': p_value_age,
                        'z_statistic': z_stat_age,
                        'control_rate': (young_conversions / young_impressions) * 100,
                        'treatment_rate': (older_conversions / older_impressions) * 100,
                        'significant': p_value_age < 0.05,
                        'test_name': 'Digital Ads Age Group Comparison',
                        'data_source': 'Real Digital Ads Data'
                    }

                    print(f"   Age Group Comparison: p = {p_value_age:.4f} {'âœ…' if p_value_age < 0.05 else 'âŒ'}")

    print(f"\nğŸ“Š TOTAL REAL A/B TESTS ANALYZED: {len(real_test_results)}")
    print(f"âœ… Data Source: 100% Real datasets")

    return real_test_results

def apply_multiple_testing_corrections_real(real_test_results):
    """
    Apply multiple testing corrections to your REAL A/B test p-values
    """
    print("\nğŸ”¬ MULTIPLE TESTING CORRECTIONS - REAL DATA")
    print("=" * 55)

    if not real_test_results:
        print("âŒ No real test results to analyze")
        return None

    # Extract real p-values
    test_names = list(real_test_results.keys())
    p_values = [real_test_results[test]['p_value'] for test in test_names]
    n_tests = len(p_values)

    print(f"ğŸ“Š REAL P-VALUES FROM YOUR TESTS:")
    for i, test_name in enumerate(test_names):
        significant = "âœ…" if p_values[i] < 0.05 else "âŒ"
        print(f"   {test_name}: p = {p_values[i]:.4f} {significant}")

    # Apply corrections
    corrections_results = {}

    # 1. Bonferroni Correction
    bonferroni_alpha = 0.05 / n_tests
    bonferroni_significant = [p < bonferroni_alpha for p in p_values]

    print(f"\nğŸ”¬ BONFERRONI CORRECTION:")
    print(f"   Original Î±: 0.05")
    print(f"   Corrected Î±: {bonferroni_alpha:.4f}")
    print(f"   Significant tests: {sum(bonferroni_significant)}/{n_tests}")

    for i, test_name in enumerate(test_names):
        status = "âœ… Significant" if bonferroni_significant[i] else "âŒ Not Significant"
        print(f"   {test_name}: {status}")

    # 2. Benjamini-Hochberg (FDR)
    bh_significant, bh_pvalues_corrected, _, _ = multipletests(
        p_values, alpha=0.05, method='fdr_bh'
    )

    print(f"\nğŸ”¬ BENJAMINI-HOCHBERG CORRECTION (FDR):")
    print(f"   FDR Level: 0.05")
    print(f"   Significant tests: {sum(bh_significant)}/{n_tests}")

    for i, test_name in enumerate(test_names):
        status = "âœ… Significant" if bh_significant[i] else "âŒ Not Significant"
        print(f"   {test_name}: {status} (corrected p = {bh_pvalues_corrected[i]:.4f})")

    # 3. Holm Correction
    holm_significant, holm_pvalues_corrected, _, _ = multipletests(
        p_values, alpha=0.05, method='holm'
    )

    print(f"\nğŸ”¬ HOLM CORRECTION:")
    print(f"   Significant tests: {sum(holm_significant)}/{n_tests}")

    for i, test_name in enumerate(test_names):
        status = "âœ… Significant" if holm_significant[i] else "âŒ Not Significant"
        print(f"   {test_name}: {status}")

    # Summary
    original_significant = sum(1 for p in p_values if p < 0.05)

    corrections_results = {
        'original_p_values': p_values,
        'test_names': test_names,
        'original_significant': original_significant,
        'bonferroni': {
            'corrected_alpha': bonferroni_alpha,
            'significant': bonferroni_significant,
            'num_significant': sum(bonferroni_significant)
        },
        'benjamini_hochberg': {
            'significant': bh_significant.tolist(),
            'corrected_pvalues': bh_pvalues_corrected.tolist(),
            'num_significant': sum(bh_significant)
        },
        'holm': {
            'significant': holm_significant.tolist(),
            'corrected_pvalues': holm_pvalues_corrected.tolist(),
            'num_significant': sum(holm_significant)
        }
    }

    print(f"\nğŸ“Š CORRECTION SUMMARY:")
    print(f"   Original significant: {original_significant}/{n_tests}")
    print(f"   Bonferroni significant: {corrections_results['bonferroni']['num_significant']}/{n_tests}")
    print(f"   Benjamini-Hochberg significant: {corrections_results['benjamini_hochberg']['num_significant']}/{n_tests}")
    print(f"   Holm significant: {corrections_results['holm']['num_significant']}/{n_tests}")

    print(f"\nğŸ¯ DATA SOURCE: 100% Real A/B Test Results")

    return corrections_results

def meta_analysis_real_data(ab_tester):
    """
    Perform meta-analysis using subsamples of your real Cookie Cats data
    """
    print("\nğŸ“Š META-ANALYSIS USING REAL COOKIE CATS DATA")
    print("=" * 50)

    if 'cookie_cats' not in ab_tester.datasets:
        print("âŒ Cookie Cats data not available")
        return None

    data = ab_tester.datasets['cookie_cats']

    # Create multiple "studies" by subsampling your real data
    studies = []
    study_names = []

    # Split data into 4 subsamples to simulate different studies
    sample_size = len(data) // 4

    for i in range(4):
        start_idx = i * sample_size
        end_idx = (i + 1) * sample_size if i < 3 else len(data)

        subsample = data.iloc[start_idx:end_idx]

        control_data = subsample[subsample['version'] == 'gate_30']
        treatment_data = subsample[subsample['version'] == 'gate_40']

        control_rate = control_data['retention_1'].mean()
        treatment_rate = treatment_data['retention_1'].mean()

        studies.append({
            'study_name': f'Cookie_Cats_Subsample_{i+1}',
            'control_rate': control_rate,
            'treatment_rate': treatment_rate,
            'control_n': len(control_data),
            'treatment_n': len(treatment_data)
        })
        study_names.append(f'Subsample_{i+1}')

        print(f"   Study {i+1}: Control={control_rate*100:.2f}%, Treatment={treatment_rate*100:.2f}%")

    # Simple meta-analysis (mean of effect sizes)
    effect_sizes = []
    for study in studies:
        if study['control_rate'] > 0:
            relative_effect = (study['treatment_rate'] - study['control_rate']) / study['control_rate']
            effect_sizes.append(relative_effect)

    if effect_sizes:
        pooled_effect = np.mean(effect_sizes)
        effect_std = np.std(effect_sizes)

        print(f"\nğŸ“ˆ META-ANALYSIS RESULTS:")
        print(f"   Number of studies: {len(studies)}")
        print(f"   Pooled effect size: {pooled_effect*100:+.2f}%")
        print(f"   Effect size std: {effect_std*100:.2f}%")
        print(f"   Consistent direction: {'Yes' if all(e < 0 for e in effect_sizes) else 'No'}")

        return {
            'studies': studies,
            'effect_sizes': effect_sizes,
            'pooled_effect': pooled_effect,
            'effect_std': effect_std,
            'num_studies': len(studies)
        }

    return None

def run_real_multiple_testing_analysis(ab_tester):
    """
    Run complete multiple testing analysis using ONLY real data
    """
    print("ğŸš€ REAL DATA MULTIPLE TESTING ANALYSIS")
    print("=" * 60)
    print("ğŸ¯ Using ONLY your real datasets")
    print("âŒ No synthetic data")
    print("âŒ No simulation")
    print("âœ… 100% authentic A/B test results")
    print("=" * 60)

    # 1. Analyze multiple real A/B tests
    real_test_results = analyze_multiple_real_ab_tests(ab_tester)

    if not real_test_results:
        print("âŒ No real A/B test results found")
        return None

    # 2. Apply multiple testing corrections
    correction_results = apply_multiple_testing_corrections_real(real_test_results)

    # 3. Meta-analysis using real data
    meta_results = meta_analysis_real_data(ab_tester)

    # 4. Summary
    print(f"\nğŸ‰ MULTIPLE TESTING ANALYSIS COMPLETE")
    print("=" * 45)

    num_tests = len(real_test_results)
    original_significant = sum(1 for test in real_test_results.values() if test['significant'])

    if correction_results:
        bonf_significant = correction_results['bonferroni']['num_significant']
        bh_significant = correction_results['benjamini_hochberg']['num_significant']

        print(f"ğŸ“Š RESULTS SUMMARY:")
        print(f"   Total real A/B tests: {num_tests}")
        print(f"   Originally significant: {original_significant}")
        print(f"   After Bonferroni: {bonf_significant}")
        print(f"   After Benjamini-Hochberg: {bh_significant}")

        if meta_results:
            print(f"   Meta-analysis pooled effect: {meta_results['pooled_effect']*100:+.1f}%")

    print(f"\nğŸ¯ DATA AUTHENTICITY:")
    print(f"   âœ… Real datasets: {len(ab_tester.datasets)}")
    print(f"   âœ… Real A/B tests: {num_tests}")
    print(f"   âœ… Real statistical results: 100%")
    print(f"   âŒ Synthetic data: 0%")

    return {
        'real_test_results': real_test_results,
        'correction_results': correction_results,
        'meta_analysis': meta_results
    }

def create_real_multiple_testing_visualization(results):
    """Create visualizations for real multiple testing analysis"""

    if not results or 'real_test_results' not in results:
        print("âŒ No results to visualize")
        return None

    print("ğŸ“Š CREATING REAL DATA VISUALIZATION")
    print("=" * 40)

    real_tests = results['real_test_results']
    correction_results = results.get('correction_results')

    # Create subplots
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            'P-values from Real A/B Tests',
            'Correction Method Comparison',
            'Effect Sizes by Test',
            'Significance Before/After Correction'
        )
    )

    # P-values from real tests
    test_names = [test['test_name'] for test in real_tests.values()]
    p_values = [test['p_value'] for test in real_tests.values()]

    fig.add_trace(
        go.Bar(
            x=test_names,
            y=p_values,
            name='P-values',
            marker_color=['green' if p < 0.05 else 'red' for p in p_values]
        ),
        row=1, col=1
    )

    # Add significance line
    fig.add_hline(y=0.05, line_dash="dash", line_color="red",
                 annotation_text="Î± = 0.05", row=1, col=1)

    # Correction comparison
    if correction_results:
        methods = ['Original', 'Bonferroni', 'Benjamini-Hochberg', 'Holm']
        significant_counts = [
            correction_results['original_significant'],
            correction_results['bonferroni']['num_significant'],
            correction_results['benjamini_hochberg']['num_significant'],
            correction_results['holm']['num_significant']
        ]

        fig.add_trace(
            go.Bar(
                x=methods,
                y=significant_counts,
                name='Significant Tests',
                marker_color=['blue', 'red', 'green', 'orange']
            ),
            row=1, col=2
        )

    # Effect sizes
    effect_sizes = []
    for test in real_tests.values():
        control_rate = test['control_rate']
        treatment_rate = test['treatment_rate']
        if control_rate > 0:
            effect = ((treatment_rate - control_rate) / control_rate) * 100
            effect_sizes.append(effect)
        else:
            effect_sizes.append(0)

    fig.add_trace(
        go.Bar(
            x=test_names,
            y=effect_sizes,
            name='Effect Size (%)',
            marker_color=['green' if e > 0 else 'red' for e in effect_sizes]
        ),
        row=2, col=1
    )

    fig.update_layout(
        title="Real Data Multiple Testing Analysis",
        showlegend=True,
        height=800
    )

    return fig

# ==========================================
# MAIN EXECUTION - RUN THIS
# ==========================================

print("ğŸ”¬ CHUNK 5 - REAL DATA MULTIPLE TESTING ANALYSIS")
print("=" * 55)
print("âœ… Fixed method errors")
print("âœ… Uses only real datasets")
print("âœ… No synthetic or simulated data")
print("\nUsage:")
print("# Run this to analyze multiple real A/B tests:")
print("real_multiple_results = run_real_multiple_testing_analysis(ab_tester)")
print("# Create visualization:")
print("mt_viz = create_real_multiple_testing_visualization(real_multiple_results)")

real_multiple_results = run_real_multiple_testing_analysis(ab_tester)

mt_viz = create_real_multiple_testing_visualization(real_multiple_results)

# CHUNK 6: REAL DATA INTEGRATION & COMPREHENSIVE ANALYSIS
# Complete analysis pipeline using your actual datasets

def add_comprehensive_analysis_methods(ab_tester_class):
    """Add comprehensive analysis methods that integrate all previous chunks"""

    def comprehensive_ab_analysis(self, dataset_name='cookie_cats', metrics_list=None,
                                analysis_type='full', confidence_level=0.95):
        """
        Run complete A/B testing analysis using all advanced methods on your real data

        Parameters:
        - dataset_name: 'cookie_cats', 'ab_facebook', or 'digital_ads'
        - metrics_list: List of metrics to analyze (None = auto-detect)
        - analysis_type: 'full', 'quick', 'bayesian_only', 'frequentist_only'
        - confidence_level: Confidence/credible interval level
        """

        print("ğŸš€ COMPREHENSIVE A/B TESTING ANALYSIS")
        print("=" * 60)
        print(f"ğŸ¯ Dataset: {dataset_name.upper()}")
        print(f"ğŸ“Š Analysis Type: {analysis_type.upper()}")
        print(f"ğŸ” Confidence Level: {confidence_level*100:.0f}%")
        print("=" * 60)

        if dataset_name not in self.datasets:
            print(f"âŒ Dataset '{dataset_name}' not loaded")
            return None

        # Auto-detect metrics if not provided
        if metrics_list is None:
            metrics_list = self._auto_detect_metrics(dataset_name)

        print(f"ğŸ“ˆ Metrics to Analyze: {', '.join(metrics_list)}")
        print("-" * 60)

        # Initialize results container
        comprehensive_results = {
            'dataset': dataset_name,
            'metrics': metrics_list,
            'analysis_timestamp': datetime.now().isoformat(),
            'analysis_type': analysis_type,
            'confidence_level': confidence_level,
            'summary': {},
            'detailed_results': {}
        }

        # Run analysis for each metric
        for i, metric in enumerate(metrics_list):
            print(f"\nğŸ“Š ANALYZING METRIC {i+1}/{len(metrics_list)}: {metric.upper()}")
            print("=" * 50)

            metric_results = self._analyze_single_metric(
                dataset_name, metric, analysis_type, confidence_level
            )

            if metric_results:
                comprehensive_results['detailed_results'][metric] = metric_results
                comprehensive_results['summary'][metric] = self._extract_metric_summary(metric_results)

        # Multi-metric analysis
        if len(metrics_list) > 1:
            print(f"\nğŸ”„ MULTI-METRIC ANALYSIS")
            print("=" * 30)

            multi_metric_results = self._multi_metric_analysis(
                dataset_name, metrics_list, comprehensive_results['detailed_results']
            )
            comprehensive_results['multi_metric_analysis'] = multi_metric_results

        # Generate executive summary
        executive_summary = self._generate_executive_summary(comprehensive_results)
        comprehensive_results['executive_summary'] = executive_summary

        # Display executive summary
        print(f"\nğŸ“‹ EXECUTIVE SUMMARY")
        print("=" * 25)
        self._display_executive_summary(executive_summary)

        return comprehensive_results

    def _auto_detect_metrics(self, dataset_name):
        """Auto-detect available metrics for each dataset"""

        if dataset_name == 'cookie_cats':
            return ['retention_1', 'retention_7']
        elif dataset_name == 'ab_facebook':
            return ['purchase_rate', 'click_rate']
        elif dataset_name == 'digital_ads':
            return ['conversion_rate', 'click_rate']
        else:
            return ['conversion_rate']  # Default

    def _analyze_single_metric(self, dataset_name, metric, analysis_type, confidence_level):
        """Analyze a single metric using all available methods"""

        results = {
            'metric': metric,
            'dataset': dataset_name
        }

        try:
            # 1. Basic descriptive statistics
            desc_stats = self._calculate_descriptive_stats(dataset_name, metric)
            results['descriptive_stats'] = desc_stats

            # 2. Frequentist analysis (always included)
            if analysis_type in ['full', 'quick', 'frequentist_only']:
                freq_results = self._run_frequentist_analysis(dataset_name, metric, confidence_level)
                results['frequentist'] = freq_results

            # 3. Bayesian analysis
            if analysis_type in ['full', 'bayesian_only']:
                bayesian_results = self.bayesian_ab_analysis(
                    dataset_name, metric,
                    credible_interval=confidence_level,
                    n_simulations=50000
                )
                results['bayesian'] = bayesian_results

            # 4. Power analysis (for full analysis)
            if analysis_type == 'full':
                power_results = self._run_power_analysis(dataset_name, metric)
                results['power_analysis'] = power_results

            # 5. Sequential testing simulation (for full analysis)
            if analysis_type == 'full':
                sequential_results = self._run_sequential_simulation(dataset_name, metric)
                results['sequential_testing'] = sequential_results

            print(f"âœ… Analysis complete for {metric}")

        except Exception as e:
            print(f"âŒ Error analyzing {metric}: {str(e)}")
            results['error'] = str(e)

        return results

    def _calculate_descriptive_stats(self, dataset_name, metric):
        """Calculate descriptive statistics for the metric"""

        control_data, treatment_data = self._extract_ab_data(dataset_name, metric)

        if control_data is None or treatment_data is None:
            return None

        control_rate = control_data['conversions'] / control_data['total']
        treatment_rate = treatment_data['conversions'] / treatment_data['total']

        return {
            'control': {
                'conversions': control_data['conversions'],
                'total': control_data['total'],
                'rate': control_rate,
                'rate_pct': control_rate * 100
            },
            'treatment': {
                'conversions': treatment_data['conversions'],
                'total': treatment_data['total'],
                'rate': treatment_rate,
                'rate_pct': treatment_rate * 100
            },
            'difference': {
                'absolute': treatment_rate - control_rate,
                'relative': ((treatment_rate - control_rate) / control_rate * 100) if control_rate > 0 else 0,
                'relative_pct': ((treatment_rate - control_rate) / control_rate * 100) if control_rate > 0 else 0
            }
        }

    def _run_frequentist_analysis(self, dataset_name, metric, confidence_level):
        """Run frequentist statistical analysis"""

        control_data, treatment_data = self._extract_ab_data(dataset_name, metric)

        if control_data is None or treatment_data is None:
            return None

        # Two-proportion z-test
        z_stat, p_value = proportions_ztest(
            [control_data['conversions'], treatment_data['conversions']],
            [control_data['total'], treatment_data['total']]
        )

        # Effect size (Cohen's h)
        control_rate = control_data['conversions'] / control_data['total']
        treatment_rate = treatment_data['conversions'] / treatment_data['total']

        effect_size = proportion_effectsize(control_rate, treatment_rate)

        # Confidence interval for difference in proportions
        alpha = 1 - confidence_level
        z_critical = norm.ppf(1 - alpha/2)

        p1, p2 = control_rate, treatment_rate
        n1, n2 = control_data['total'], treatment_data['total']

        se_diff = np.sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)
        diff = p2 - p1

        ci_lower = diff - z_critical * se_diff
        ci_upper = diff + z_critical * se_diff

        return {
            'z_statistic': z_stat,
            'p_value': p_value,
            'effect_size_cohens_h': effect_size,
            'significant': p_value < (1 - confidence_level),
            'confidence_interval': {
                'lower': ci_lower * 100,
                'upper': ci_upper * 100,
                'level': confidence_level * 100
            },
            'statistical_power': None  # Would need effect size assumption
        }

    def _run_power_analysis(self, dataset_name, metric):
        """Run power analysis for the metric"""

        try:
            # Use existing method
            power_results = self.calculate_sample_size_for_your_data(
                dataset_name=dataset_name,
                metric=metric,
                mde=0.15,
                power=0.8,
                alpha=0.05
            )
            return power_results
        except Exception as e:
            return {'error': str(e)}

    def _run_sequential_simulation(self, dataset_name, metric):
        """Run sequential testing simulation"""

        try:
            # Run a quick simulation
            seq_history = self.simulate_sequential_test(
                dataset_name=dataset_name,
                metric=metric,
                max_days=10,
                spending_function='obrien_fleming'
            )

            if seq_history:
                final_result = seq_history[-1]
                return {
                    'days_to_decision': len(seq_history),
                    'final_recommendation': final_result['recommendation'],
                    'stopped_early': final_result['stop_for_efficacy'] or final_result['stop_for_futility'],
                    'final_p_value': final_result['p_value']
                }
        except Exception as e:
            return {'error': str(e)}

        return None

    def _extract_metric_summary(self, metric_results):
        """Extract key summary statistics from detailed results"""

        summary = {
            'metric': metric_results['metric']
        }

        # Descriptive stats
        if 'descriptive_stats' in metric_results:
            desc = metric_results['descriptive_stats']
            summary['control_rate'] = desc['control']['rate_pct']
            summary['treatment_rate'] = desc['treatment']['rate_pct']
            summary['relative_change'] = desc['difference']['relative_pct']

        # Frequentist results
        if 'frequentist' in metric_results:
            freq = metric_results['frequentist']
            summary['p_value'] = freq['p_value']
            summary['significant_frequentist'] = freq['significant']

        # Bayesian results
        if 'bayesian' in metric_results:
            bayes = metric_results['bayesian']
            summary['prob_treatment_better'] = bayes['probabilities']['treatment_better']
            summary['bayesian_recommendation'] = bayes['recommendation']

        return summary

    def _multi_metric_analysis(self, dataset_name, metrics_list, detailed_results):
        """Analyze relationships between multiple metrics"""

        print("ğŸ”„ Running multi-metric correlation analysis...")

        # Extract p-values for multiple testing correction
        p_values_dict = {}
        for metric in metrics_list:
            if metric in detailed_results and 'frequentist' in detailed_results[metric]:
                p_values_dict[metric] = detailed_results[metric]['frequentist']

        # Apply multiple testing corrections
        corrections = {}
        if len(p_values_dict) > 1:
            for method in ['bonferroni', 'benjamini_hochberg']:
                correction_result = self.multiple_testing_correction(
                    p_values_dict,
                    correction_method=method,
                    alpha=0.05
                )
                corrections[method] = correction_result

        # Metric correlations (if Bayesian samples available)
        correlations = {}
        if len(metrics_list) > 1:
            for i, metric1 in enumerate(metrics_list):
                for metric2 in metrics_list[i+1:]:
                    if (metric1 in detailed_results and 'bayesian' in detailed_results[metric1] and
                        metric2 in detailed_results and 'bayesian' in detailed_results[metric2]):

                        samples1 = detailed_results[metric1]['bayesian']['samples']['relative_improvement']
                        samples2 = detailed_results[metric2]['bayesian']['samples']['relative_improvement']

                        correlation = np.corrcoef(samples1, samples2)[0,1]
                        correlations[f'{metric1}_vs_{metric2}'] = correlation

        return {
            'multiple_testing_corrections': corrections,
            'metric_correlations': correlations,
            'num_metrics_analyzed': len(metrics_list)
        }

    def _generate_executive_summary(self, comprehensive_results):
        """Generate executive summary of all analyses"""

        summary = {
            'dataset': comprehensive_results['dataset'],
            'total_metrics': len(comprehensive_results['metrics']),
            'analysis_timestamp': comprehensive_results['analysis_timestamp'],
            'key_findings': [],
            'recommendations': [],
            'business_impact': {},
            'statistical_confidence': 'High',
            'next_steps': []
        }

        # Analyze each metric's results
        significant_improvements = 0
        significant_degradations = 0
        inconclusive_results = 0

        for metric, results in comprehensive_results['summary'].items():

            # Determine significance and direction
            if 'significant_frequentist' in results and results['significant_frequentist']:
                if results['relative_change'] > 0:
                    significant_improvements += 1
                    summary['key_findings'].append(
                        f"âœ… {metric}: Significant +{results['relative_change']:.1f}% improvement (p={results['p_value']:.4f})"
                    )
                else:
                    significant_degradations += 1
                    summary['key_findings'].append(
                        f"âŒ {metric}: Significant {results['relative_change']:.1f}% degradation (p={results['p_value']:.4f})"
                    )
            else:
                inconclusive_results += 1
                summary['key_findings'].append(
                    f"âšª {metric}: No significant effect detected ({results['relative_change']:+.1f}%)"
                )

            # Add Bayesian insights if available
            if 'prob_treatment_better' in results:
                prob = results['prob_treatment_better']
                if prob > 95:
                    summary['key_findings'].append(
                        f"ğŸ”® {metric}: {prob:.1f}% probability treatment is better (Bayesian)"
                    )

        # Generate recommendations
        if significant_improvements > 0:
            summary['recommendations'].append(
                f"ğŸš€ IMPLEMENT: {significant_improvements} metric(s) show significant improvement"
            )

        if significant_degradations > 0:
            summary['recommendations'].append(
                f"ğŸ›‘ AVOID: {significant_degradations} metric(s) show significant degradation"
            )

        if inconclusive_results > 0:
            summary['recommendations'].append(
                f"ğŸ” INVESTIGATE: {inconclusive_results} metric(s) need more data or longer testing"
            )

        # Multiple testing considerations
        if 'multi_metric_analysis' in comprehensive_results:
            multi_results = comprehensive_results['multi_metric_analysis']
            if 'multiple_testing_corrections' in multi_results:
                summary['next_steps'].append(
                    "ğŸ“Š Consider multiple testing corrections when evaluating significance"
                )

        # Business impact estimation
        summary['business_impact'] = self._estimate_overall_business_impact(
            comprehensive_results['dataset'], comprehensive_results['summary']
        )

        return summary

    def _estimate_overall_business_impact(self, dataset_name, metric_summaries):
        """Estimate overall business impact across all metrics"""

        impact = {
            'primary_metric_impact': 'TBD',
            'confidence_level': 'Medium',
            'risk_assessment': 'Low',
            'implementation_priority': 'Medium'
        }

        # Find primary metric (first one with significant positive effect)
        primary_metric = None
        for metric, summary in metric_summaries.items():
            if (summary.get('significant_frequentist', False) and
                summary.get('relative_change', 0) > 0):
                primary_metric = metric
                break

        if primary_metric:
            relative_change = metric_summaries[primary_metric]['relative_change']

            if dataset_name == 'cookie_cats':
                total_users = len(self.datasets['cookie_cats'])
                baseline_rate = self._get_baseline_rate(dataset_name, primary_metric)
                if baseline_rate:
                    additional_users = total_users * baseline_rate * (relative_change / 100)
                    impact['primary_metric_impact'] = f"{additional_users:,.0f} additional retained users"
                    impact['estimated_revenue'] = f"${additional_users * 5:,.0f} (at $5/user)"

            # Set implementation priority
            if relative_change > 10:
                impact['implementation_priority'] = 'High'
            elif relative_change > 5:
                impact['implementation_priority'] = 'Medium'
            else:
                impact['implementation_priority'] = 'Low'

        return impact

    def _display_executive_summary(self, summary):
        """Display the executive summary in a formatted way"""

        print(f"ğŸ“Š Dataset: {summary['dataset']}")
        print(f"ğŸ“ˆ Metrics Analyzed: {summary['total_metrics']}")
        print(f"ğŸ” Statistical Confidence: {summary['statistical_confidence']}")

        print(f"\nğŸ¯ KEY FINDINGS:")
        for finding in summary['key_findings']:
            print(f"   {finding}")

        print(f"\nğŸ“‹ RECOMMENDATIONS:")
        for rec in summary['recommendations']:
            print(f"   {rec}")

        if summary['business_impact']:
            print(f"\nğŸ’¼ BUSINESS IMPACT:")
            for key, value in summary['business_impact'].items():
                print(f"   {key.replace('_', ' ').title()}: {value}")

        if summary['next_steps']:
            print(f"\nğŸ“Œ NEXT STEPS:")
            for step in summary['next_steps']:
                print(f"   {step}")

    def generate_ab_test_report(self, comprehensive_results, include_technical_details=False):
        """Generate a complete A/B test report"""

        print("ğŸ“‹ GENERATING A/B TEST REPORT")
        print("=" * 40)

        report = {
            'title': f"A/B Test Analysis Report - {comprehensive_results['dataset']}",
            'executive_summary': comprehensive_results['executive_summary'],
            'methodology': self._generate_methodology_section(),
            'results_by_metric': {},
            'conclusions': self._generate_conclusions(comprehensive_results),
            'technical_appendix': {} if include_technical_details else None
        }

        # Generate results section for each metric
        for metric, results in comprehensive_results['detailed_results'].items():
            report['results_by_metric'][metric] = self._format_metric_results(metric, results)

        # Technical appendix
        if include_technical_details:
            report['technical_appendix'] = {
                'statistical_methods': self._generate_methods_description(),
                'assumptions_and_limitations': self._generate_limitations(),
                'raw_data_summary': comprehensive_results['summary']
            }

        return report

    def _generate_methodology_section(self):
        """Generate methodology section for the report"""

        return {
            'test_type': 'A/B Test (Randomized Controlled Experiment)',
            'statistical_methods': [
                'Two-proportion z-test (Frequentist)',
                'Bayesian Beta-Binomial analysis',
                'Sequential testing with O\'Brien-Fleming boundaries',
                'Multiple testing corrections (Benjamini-Hochberg)'
            ],
            'significance_level': '5% (Î± = 0.05)',
            'power': '80% (Î² = 0.20)',
            'effect_size': 'Cohen\'s h for proportion differences'
        }

    def _format_metric_results(self, metric, results):
        """Format results for a single metric"""

        formatted = {
            'metric_name': metric,
            'summary': f"Analysis of {metric} conversion rates between control and treatment groups"
        }

        if 'descriptive_stats' in results:
            desc = results['descriptive_stats']
            formatted['descriptive_statistics'] = {
                'control_conversion_rate': f"{desc['control']['rate_pct']:.2f}%",
                'treatment_conversion_rate': f"{desc['treatment']['rate_pct']:.2f}%",
                'absolute_difference': f"{desc['difference']['absolute']*100:+.2f}%",
                'relative_improvement': f"{desc['difference']['relative_pct']:+.1f}%"
            }

        if 'frequentist' in results:
            freq = results['frequentist']
            formatted['statistical_test'] = {
                'test_type': 'Two-proportion z-test',
                'z_statistic': f"{freq['z_statistic']:.3f}",
                'p_value': f"{freq['p_value']:.4f}",
                'significant': 'Yes' if freq['significant'] else 'No',
                'confidence_interval': f"[{freq['confidence_interval']['lower']:+.2f}%, {freq['confidence_interval']['upper']:+.2f}%]"
            }

        if 'bayesian' in results:
            bayes = results['bayesian']
            formatted['bayesian_analysis'] = {
                'probability_treatment_better': f"{bayes['probabilities']['treatment_better']:.1f}%",
                'recommendation': bayes['recommendation'],
                'expected_loss_if_wrong': f"{min(bayes['expected_loss']['if_choose_treatment'], bayes['expected_loss']['if_choose_control']):.3f}%"
            }

        return formatted

    def _generate_conclusions(self, comprehensive_results):
        """Generate conclusions section"""

        conclusions = {
            'primary_conclusion': '',
            'secondary_findings': [],
            'business_recommendation': '',
            'confidence_level': 'Medium'
        }

        # Determine primary conclusion based on most important metric
        summary = comprehensive_results['executive_summary']

        if summary['key_findings']:
            first_finding = summary['key_findings'][0]
            if 'âœ…' in first_finding:
                conclusions['primary_conclusion'] = "Treatment variant shows statistically significant improvement"
                conclusions['business_recommendation'] = "Implement the treatment variant"
                conclusions['confidence_level'] = 'High'
            elif 'âŒ' in first_finding:
                conclusions['primary_conclusion'] = "Treatment variant shows statistically significant degradation"
                conclusions['business_recommendation'] = "Keep the control variant"
                conclusions['confidence_level'] = 'High'
            else:
                conclusions['primary_conclusion'] = "No statistically significant difference detected"
                conclusions['business_recommendation'] = "Consider longer test duration or larger sample size"
                conclusions['confidence_level'] = 'Low'

        # Add secondary findings
        if len(summary['key_findings']) > 1:
            conclusions['secondary_findings'] = summary['key_findings'][1:]

        return conclusions

    def _generate_methods_description(self):
        """Generate detailed methods description"""

        return {
            'frequentist_testing': "Two-proportion z-test with Wald confidence intervals",
            'bayesian_analysis': "Beta-Binomial conjugate priors with Monte Carlo simulation",
            'sequential_testing': "Group sequential design with O'Brien-Fleming alpha spending",
            'multiple_comparisons': "Benjamini-Hochberg false discovery rate control",
            'effect_size': "Cohen's h for standardized difference between proportions"
        }

    def _generate_limitations(self):
        """Generate limitations section"""

        return [
            "Results assume random assignment and no systematic biases",
            "Effect estimates are specific to the tested population and time period",
            "Sequential testing boundaries are approximations for finite samples",
            "Bayesian analysis uses uninformative priors",
            "Multiple testing corrections may reduce statistical power"
        ]

    # Add methods to the class
    ab_tester_class.comprehensive_ab_analysis = comprehensive_ab_analysis
    ab_tester_class._auto_detect_metrics = _auto_detect_metrics
    ab_tester_class._analyze_single_metric = _analyze_single_metric
    ab_tester_class._calculate_descriptive_stats = _calculate_descriptive_stats
    ab_tester_class._run_frequentist_analysis = _run_frequentist_analysis
    ab_tester_class._run_power_analysis = _run_power_analysis
    ab_tester_class._run_sequential_simulation = _run_sequential_simulation
    ab_tester_class._extract_metric_summary = _extract_metric_summary
    ab_tester_class._multi_metric_analysis = _multi_metric_analysis
    ab_tester_class._generate_executive_summary = _generate_executive_summary
    ab_tester_class._estimate_overall_business_impact = _estimate_overall_business_impact
    ab_tester_class._display_executive_summary = _display_executive_summary
    ab_tester_class.generate_ab_test_report = generate_ab_test_report
    ab_tester_class._generate_methodology_section = _generate_methodology_section
    ab_tester_class._format_metric_results = _format_metric_results
    ab_tester_class._generate_conclusions = _generate_conclusions
    ab_tester_class._generate_methods_description = _generate_methods_description
    ab_tester_class._generate_limitations = _generate_limitations

# Master execution function
def run_complete_ab_analysis(ab_tester, dataset_name='cookie_cats'):
    """Run the complete A/B testing analysis pipeline on your real data"""

    print("ğŸš€ COMPLETE A/B TESTING ANALYSIS PIPELINE")
    print("=" * 70)
    print("ğŸ¯ Using Your Real Datasets for Professional-Grade Analysis")
    print("=" * 70)

    # Add all methods from previous chunks
    add_comprehensive_analysis_methods(ab_tester.__class__)

    # Run comprehensive analysis
    print(f"\n1ï¸âƒ£ RUNNING COMPREHENSIVE ANALYSIS ON {dataset_name.upper()}")
    print("=" * 60)

    comprehensive_results = ab_tester.comprehensive_ab_analysis(
        dataset_name=dataset_name,
        analysis_type='full',
        confidence_level=0.95
    )

    if not comprehensive_results:
        print("âŒ Analysis failed")
        return None

    print(f"\n2ï¸âƒ£ GENERATING PROFESSIONAL REPORT")
    print("=" * 40)

    # Generate detailed report
    report = ab_tester.generate_ab_test_report(
        comprehensive_results,
        include_technical_details=True
    )

    print(f"\n3ï¸âƒ£ BUSINESS IMPACT SUMMARY")
    print("=" * 35)

    business_impact = comprehensive_results['executive_summary']['business_impact']
    for key, value in business_impact.items():
        print(f"   {key.replace('_', ' ').title()}: {value}")

    print(f"\nğŸ‰ ANALYSIS COMPLETE!")
    print("=" * 25)
    print("âœ… Frequentist statistical testing")
    print("âœ… Bayesian probability analysis")
    print("âœ… Sequential testing simulation")
    print("âœ… Power analysis & sample size calculation")
    print("âœ… Multiple testing corrections")
    print("âœ… Business impact estimation")
    print("âœ… Professional reporting")

    return {
        'comprehensive_results': comprehensive_results,
        'report': report,
        'dataset_analyzed': dataset_name
    }

# Quick analysis function for demos
def quick_ab_analysis(ab_tester, dataset_name='cookie_cats'):
    """Run a quick A/B analysis for demos and presentations"""

    print("âš¡ QUICK A/B TESTING ANALYSIS")
    print("=" * 40)

    # Add methods
    add_comprehensive_analysis_methods(ab_tester.__class__)

    # Run quick analysis
    results = ab_tester.comprehensive_ab_analysis(
        dataset_name=dataset_name,
        analysis_type='quick',
        confidence_level=0.95
    )

    if results:
        print(f"\nğŸ¯ QUICK INSIGHTS:")
        for metric, summary in results['summary'].items():
            print(f"   ğŸ“Š {metric}: {summary['relative_change']:+.1f}% change")
            if 'p_value' in summary:
                sig = "âœ… Significant" if summary['p_value'] < 0.05 else "âŒ Not significant"
                print(f"      Statistical: {sig} (p={summary['p_value']:.4f})")

    return results

if __name__ == "__main__":
    print("ğŸ¯ ENHANCED A/B TESTING FRAMEWORK - CHUNK 6")
    print("Real Data Integration & Comprehensive Analysis")
    print("\nUsage:")
    print("# Run complete analysis on your Cookie Cats data:")
    print("complete_results = run_complete_ab_analysis(ab_tester, 'cookie_cats')")
    print("# Or quick analysis for demos:")
    print("quick_results = quick_ab_analysis(ab_tester, 'cookie_cats')")

complete_results = run_complete_ab_analysis(ab_tester, 'cookie_cats')

quick_results = quick_ab_analysis(ab_tester, 'cookie_cats')

